{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded ./ai_images\\ai_0.jpg\n",
      "No URL found for photo 53897142503\n",
      "Downloaded ./ai_images\\ai_2.jpg\n",
      "No URL found for photo 53897237109\n",
      "Downloaded ./ai_images\\ai_4.jpg\n",
      "Downloaded ./ai_images\\ai_5.jpg\n",
      "Downloaded ./ai_images\\ai_6.jpg\n",
      "Downloaded ./ai_images\\ai_7.jpg\n",
      "Downloaded ./ai_images\\ai_8.jpg\n",
      "Downloaded ./ai_images\\ai_9.jpg\n",
      "Downloaded ./ai_images\\ai_10.jpg\n",
      "Downloaded ./ai_images\\ai_11.jpg\n",
      "Downloaded ./ai_images\\ai_12.jpg\n",
      "Downloaded ./ai_images\\ai_13.jpg\n",
      "Downloaded ./ai_images\\ai_14.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from flickrapi import FlickrAPI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# Flickr API 키 설정\n",
    "API_KEY = '86110563eeebe071dd6f9aa61f207e8d'\n",
    "API_SECRET = '4bbc8fea15d76009'\n",
    "flickr = FlickrAPI(API_KEY, API_SECRET, format='parsed-json')\n",
    "\n",
    "# 이미지 검색 함수\n",
    "def search_images(query, max_results=10):\n",
    "    extras = 'url_o'  # 'url_o'는 원본 이미지 URL을 포함합니다.\n",
    "    photos = flickr.photos.search(text=query, per_page=max_results, extras=extras)\n",
    "    return photos['photos']['photo']\n",
    "# 이미지 다운로드 함수\n",
    "def download_image(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image.save(save_path)\n",
    "# 검색어에 맞는 이미지 크롤링\n",
    "def crawl_images(query, max_results=10, save_dir='./images'):\n",
    "    photos = search_images(query, max_results)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    for i, photo in enumerate(photos):\n",
    "        if 'url_o' in photo:\n",
    "            url = photo['url_o']\n",
    "            save_path = os.path.join(save_dir, f'{query}_{i}.jpg')\n",
    "            download_image(url, save_path)\n",
    "            print(f'Downloaded {save_path}')\n",
    "        else:\n",
    "            print(f'No URL found for photo {photo[\"id\"]}')\n",
    "\n",
    "# 예제 실행\n",
    "search = 'ai' # 검색어 여기에 적을것\n",
    "save_dir = f'./{search}_images' # 저장폴더 이름을 검색어와 동일하게 설정\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_images(search, max_results=15, save_dir=save_dir) # 저장폴더 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
