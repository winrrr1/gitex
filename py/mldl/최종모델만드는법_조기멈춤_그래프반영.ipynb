{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/wine.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, : 12]\n",
    "y = df.iloc[:,  12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6497*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6487*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6487*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5197, 12)\n",
      "(1300, 12)\n",
      "(5197,)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath = \"./model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "modelpath = \"./model/all4/best_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patience=20 검증셋의 오차가 20번이상 낮아지지 않을 경우 학습종료하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=modelpath, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\mldltest2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 947ms/step - accuracy: 0.2140 - loss: 3.6071\n",
      "Epoch 1: val_loss improved from inf to 0.70827, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3161 - loss: 2.4996 - val_accuracy: 0.7662 - val_loss: 0.7083\n",
      "Epoch 2/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7160 - loss: 0.8066\n",
      "Epoch 2: val_loss did not improve from 0.70827\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7440 - loss: 0.8265 - val_accuracy: 0.7638 - val_loss: 0.8536\n",
      "Epoch 3/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7640 - loss: 0.9327\n",
      "Epoch 3: val_loss improved from 0.70827 to 0.56410, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7590 - loss: 0.8232 - val_accuracy: 0.7854 - val_loss: 0.5641\n",
      "Epoch 4/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7720 - loss: 0.6314\n",
      "Epoch 4: val_loss improved from 0.56410 to 0.44910, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7917 - loss: 0.5373 - val_accuracy: 0.8315 - val_loss: 0.4491\n",
      "Epoch 5/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8320 - loss: 0.4358\n",
      "Epoch 5: val_loss improved from 0.44910 to 0.36776, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8381 - loss: 0.4314 - val_accuracy: 0.8454 - val_loss: 0.3678\n",
      "Epoch 6/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8240 - loss: 0.3624\n",
      "Epoch 6: val_loss improved from 0.36776 to 0.34162, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8272 - loss: 0.3678 - val_accuracy: 0.8492 - val_loss: 0.3416\n",
      "Epoch 7/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8240 - loss: 0.3561\n",
      "Epoch 7: val_loss improved from 0.34162 to 0.30046, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8447 - loss: 0.3383 - val_accuracy: 0.8831 - val_loss: 0.3005\n",
      "Epoch 8/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8840 - loss: 0.3318\n",
      "Epoch 8: val_loss improved from 0.30046 to 0.26846, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8852 - loss: 0.3076 - val_accuracy: 0.8985 - val_loss: 0.2685\n",
      "Epoch 9/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8840 - loss: 0.3126\n",
      "Epoch 9: val_loss improved from 0.26846 to 0.24047, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8994 - loss: 0.2772 - val_accuracy: 0.9208 - val_loss: 0.2405\n",
      "Epoch 10/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9140 - loss: 0.2550\n",
      "Epoch 10: val_loss improved from 0.24047 to 0.22357, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9228 - loss: 0.2408 - val_accuracy: 0.9285 - val_loss: 0.2236\n",
      "Epoch 11/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9380 - loss: 0.2013\n",
      "Epoch 11: val_loss improved from 0.22357 to 0.21367, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9328 - loss: 0.2237 - val_accuracy: 0.9331 - val_loss: 0.2137\n",
      "Epoch 12/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9280 - loss: 0.2239\n",
      "Epoch 12: val_loss improved from 0.21367 to 0.20780, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9320 - loss: 0.2218 - val_accuracy: 0.9323 - val_loss: 0.2078\n",
      "Epoch 13/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9420 - loss: 0.2068\n",
      "Epoch 13: val_loss improved from 0.20780 to 0.20259, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9371 - loss: 0.2118 - val_accuracy: 0.9323 - val_loss: 0.2026\n",
      "Epoch 14/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9220 - loss: 0.2496\n",
      "Epoch 14: val_loss improved from 0.20259 to 0.19715, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9314 - loss: 0.2132 - val_accuracy: 0.9308 - val_loss: 0.1972\n",
      "Epoch 15/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9360 - loss: 0.1993\n",
      "Epoch 15: val_loss improved from 0.19715 to 0.19335, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9356 - loss: 0.2003 - val_accuracy: 0.9331 - val_loss: 0.1934\n",
      "Epoch 16/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9460 - loss: 0.1641\n",
      "Epoch 16: val_loss improved from 0.19335 to 0.18977, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9384 - loss: 0.1904 - val_accuracy: 0.9315 - val_loss: 0.1898\n",
      "Epoch 17/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9320 - loss: 0.1843\n",
      "Epoch 17: val_loss improved from 0.18977 to 0.18412, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9372 - loss: 0.1903 - val_accuracy: 0.9354 - val_loss: 0.1841\n",
      "Epoch 18/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9520 - loss: 0.1689\n",
      "Epoch 18: val_loss did not improve from 0.18412\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9428 - loss: 0.1783 - val_accuracy: 0.9408 - val_loss: 0.1853\n",
      "Epoch 19/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9640 - loss: 0.1507\n",
      "Epoch 19: val_loss improved from 0.18412 to 0.17875, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9457 - loss: 0.1837 - val_accuracy: 0.9408 - val_loss: 0.1787\n",
      "Epoch 20/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9500 - loss: 0.1746\n",
      "Epoch 20: val_loss improved from 0.17875 to 0.17757, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9424 - loss: 0.1825 - val_accuracy: 0.9323 - val_loss: 0.1776\n",
      "Epoch 21/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9320 - loss: 0.1728\n",
      "Epoch 21: val_loss improved from 0.17757 to 0.17496, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9392 - loss: 0.1800 - val_accuracy: 0.9415 - val_loss: 0.1750\n",
      "Epoch 22/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9400 - loss: 0.2229\n",
      "Epoch 22: val_loss improved from 0.17496 to 0.17159, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9418 - loss: 0.1916 - val_accuracy: 0.9400 - val_loss: 0.1716\n",
      "Epoch 23/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9340 - loss: 0.1986\n",
      "Epoch 23: val_loss improved from 0.17159 to 0.16917, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9406 - loss: 0.1781 - val_accuracy: 0.9400 - val_loss: 0.1692\n",
      "Epoch 24/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9540 - loss: 0.1527\n",
      "Epoch 24: val_loss improved from 0.16917 to 0.16689, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9436 - loss: 0.1727 - val_accuracy: 0.9400 - val_loss: 0.1669\n",
      "Epoch 25/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9240 - loss: 0.1977\n",
      "Epoch 25: val_loss improved from 0.16689 to 0.16503, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9368 - loss: 0.1787 - val_accuracy: 0.9408 - val_loss: 0.1650\n",
      "Epoch 26/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9320 - loss: 0.1881\n",
      "Epoch 26: val_loss improved from 0.16503 to 0.16296, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9415 - loss: 0.1758 - val_accuracy: 0.9415 - val_loss: 0.1630\n",
      "Epoch 27/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9400 - loss: 0.2132\n",
      "Epoch 27: val_loss did not improve from 0.16296\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9396 - loss: 0.1815 - val_accuracy: 0.9415 - val_loss: 0.1634\n",
      "Epoch 28/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9460 - loss: 0.1615\n",
      "Epoch 28: val_loss improved from 0.16296 to 0.16079, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9426 - loss: 0.1666 - val_accuracy: 0.9415 - val_loss: 0.1608\n",
      "Epoch 29/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9460 - loss: 0.1671\n",
      "Epoch 29: val_loss improved from 0.16079 to 0.15849, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9422 - loss: 0.1688 - val_accuracy: 0.9415 - val_loss: 0.1585\n",
      "Epoch 30/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9380 - loss: 0.1441\n",
      "Epoch 30: val_loss improved from 0.15849 to 0.15505, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9416 - loss: 0.1625 - val_accuracy: 0.9415 - val_loss: 0.1550\n",
      "Epoch 31/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9420 - loss: 0.1490\n",
      "Epoch 31: val_loss improved from 0.15505 to 0.15411, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9460 - loss: 0.1541 - val_accuracy: 0.9423 - val_loss: 0.1541\n",
      "Epoch 32/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9460 - loss: 0.1566\n",
      "Epoch 32: val_loss improved from 0.15411 to 0.15239, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9498 - loss: 0.1548 - val_accuracy: 0.9438 - val_loss: 0.1524\n",
      "Epoch 33/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9580 - loss: 0.1195\n",
      "Epoch 33: val_loss did not improve from 0.15239\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9521 - loss: 0.1437 - val_accuracy: 0.9438 - val_loss: 0.1544\n",
      "Epoch 34/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9380 - loss: 0.1812\n",
      "Epoch 34: val_loss did not improve from 0.15239\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9457 - loss: 0.1679 - val_accuracy: 0.9423 - val_loss: 0.1537\n",
      "Epoch 35/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9500 - loss: 0.1438\n",
      "Epoch 35: val_loss improved from 0.15239 to 0.15159, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9451 - loss: 0.1521 - val_accuracy: 0.9462 - val_loss: 0.1516\n",
      "Epoch 36/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9340 - loss: 0.1945\n",
      "Epoch 36: val_loss improved from 0.15159 to 0.14620, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9456 - loss: 0.1643 - val_accuracy: 0.9431 - val_loss: 0.1462\n",
      "Epoch 37/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9620 - loss: 0.1218\n",
      "Epoch 37: val_loss did not improve from 0.14620\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9500 - loss: 0.1493 - val_accuracy: 0.9446 - val_loss: 0.1465\n",
      "Epoch 38/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9580 - loss: 0.1261\n",
      "Epoch 38: val_loss did not improve from 0.14620\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9470 - loss: 0.1478 - val_accuracy: 0.9492 - val_loss: 0.1487\n",
      "Epoch 39/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9500 - loss: 0.1408\n",
      "Epoch 39: val_loss improved from 0.14620 to 0.14193, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9476 - loss: 0.1486 - val_accuracy: 0.9438 - val_loss: 0.1419\n",
      "Epoch 40/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9540 - loss: 0.1382\n",
      "Epoch 40: val_loss did not improve from 0.14193\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9509 - loss: 0.1497 - val_accuracy: 0.9446 - val_loss: 0.1446\n",
      "Epoch 41/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9280 - loss: 0.1851\n",
      "Epoch 41: val_loss did not improve from 0.14193\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9450 - loss: 0.1532 - val_accuracy: 0.9515 - val_loss: 0.1432\n",
      "Epoch 42/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9540 - loss: 0.1569\n",
      "Epoch 42: val_loss improved from 0.14193 to 0.13932, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9519 - loss: 0.1497 - val_accuracy: 0.9462 - val_loss: 0.1393\n",
      "Epoch 43/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9440 - loss: 0.1357\n",
      "Epoch 43: val_loss improved from 0.13932 to 0.13779, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9499 - loss: 0.1380 - val_accuracy: 0.9515 - val_loss: 0.1378\n",
      "Epoch 44/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9740 - loss: 0.0924\n",
      "Epoch 44: val_loss improved from 0.13779 to 0.13736, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9550 - loss: 0.1284 - val_accuracy: 0.9508 - val_loss: 0.1374\n",
      "Epoch 45/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9640 - loss: 0.1262\n",
      "Epoch 45: val_loss improved from 0.13736 to 0.13403, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.1405 - val_accuracy: 0.9485 - val_loss: 0.1340\n",
      "Epoch 46/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9440 - loss: 0.1395\n",
      "Epoch 46: val_loss improved from 0.13403 to 0.13312, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9516 - loss: 0.1405 - val_accuracy: 0.9485 - val_loss: 0.1331\n",
      "Epoch 47/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9380 - loss: 0.1440\n",
      "Epoch 47: val_loss did not improve from 0.13312\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9477 - loss: 0.1426 - val_accuracy: 0.9469 - val_loss: 0.1342\n",
      "Epoch 48/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9520 - loss: 0.1405\n",
      "Epoch 48: val_loss improved from 0.13312 to 0.13043, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9506 - loss: 0.1371 - val_accuracy: 0.9515 - val_loss: 0.1304\n",
      "Epoch 49/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9580 - loss: 0.0964\n",
      "Epoch 49: val_loss did not improve from 0.13043\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9536 - loss: 0.1238 - val_accuracy: 0.9531 - val_loss: 0.1319\n",
      "Epoch 50/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9500 - loss: 0.1283\n",
      "Epoch 50: val_loss improved from 0.13043 to 0.12868, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9535 - loss: 0.1350 - val_accuracy: 0.9485 - val_loss: 0.1287\n",
      "Epoch 51/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9640 - loss: 0.1161\n",
      "Epoch 51: val_loss improved from 0.12868 to 0.12720, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9571 - loss: 0.1266 - val_accuracy: 0.9538 - val_loss: 0.1272\n",
      "Epoch 52/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9540 - loss: 0.1210\n",
      "Epoch 52: val_loss did not improve from 0.12720\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9563 - loss: 0.1268 - val_accuracy: 0.9492 - val_loss: 0.1273\n",
      "Epoch 53/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9360 - loss: 0.1596\n",
      "Epoch 53: val_loss improved from 0.12720 to 0.12552, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9487 - loss: 0.1382 - val_accuracy: 0.9546 - val_loss: 0.1255\n",
      "Epoch 54/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1240\n",
      "Epoch 54: val_loss improved from 0.12552 to 0.12488, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9523 - loss: 0.1261 - val_accuracy: 0.9554 - val_loss: 0.1249\n",
      "Epoch 55/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9640 - loss: 0.1152\n",
      "Epoch 55: val_loss improved from 0.12488 to 0.12452, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9574 - loss: 0.1277 - val_accuracy: 0.9508 - val_loss: 0.1245\n",
      "Epoch 56/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1240\n",
      "Epoch 56: val_loss improved from 0.12452 to 0.12230, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9567 - loss: 0.1240 - val_accuracy: 0.9515 - val_loss: 0.1223\n",
      "Epoch 57/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9540 - loss: 0.1041\n",
      "Epoch 57: val_loss improved from 0.12230 to 0.12129, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9515 - loss: 0.1256 - val_accuracy: 0.9538 - val_loss: 0.1213\n",
      "Epoch 58/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9480 - loss: 0.1329\n",
      "Epoch 58: val_loss did not improve from 0.12129\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9542 - loss: 0.1251 - val_accuracy: 0.9569 - val_loss: 0.1218\n",
      "Epoch 59/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.1285\n",
      "Epoch 59: val_loss improved from 0.12129 to 0.11995, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9579 - loss: 0.1347 - val_accuracy: 0.9523 - val_loss: 0.1199\n",
      "Epoch 60/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9540 - loss: 0.1269\n",
      "Epoch 60: val_loss improved from 0.11995 to 0.11969, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9551 - loss: 0.1277 - val_accuracy: 0.9515 - val_loss: 0.1197\n",
      "Epoch 61/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9600 - loss: 0.1147\n",
      "Epoch 61: val_loss improved from 0.11969 to 0.11763, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9520 - loss: 0.1314 - val_accuracy: 0.9569 - val_loss: 0.1176\n",
      "Epoch 62/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9580 - loss: 0.1264\n",
      "Epoch 62: val_loss improved from 0.11763 to 0.11663, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9550 - loss: 0.1314 - val_accuracy: 0.9569 - val_loss: 0.1166\n",
      "Epoch 63/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9500 - loss: 0.1200\n",
      "Epoch 63: val_loss improved from 0.11663 to 0.11588, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9560 - loss: 0.1209 - val_accuracy: 0.9585 - val_loss: 0.1159\n",
      "Epoch 64/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9420 - loss: 0.1360\n",
      "Epoch 64: val_loss improved from 0.11588 to 0.11479, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9536 - loss: 0.1222 - val_accuracy: 0.9577 - val_loss: 0.1148\n",
      "Epoch 65/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9620 - loss: 0.1034\n",
      "Epoch 65: val_loss did not improve from 0.11479\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9574 - loss: 0.1248 - val_accuracy: 0.9508 - val_loss: 0.1233\n",
      "Epoch 66/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9500 - loss: 0.1370\n",
      "Epoch 66: val_loss improved from 0.11479 to 0.11466, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9566 - loss: 0.1248 - val_accuracy: 0.9531 - val_loss: 0.1147\n",
      "Epoch 67/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9480 - loss: 0.1354\n",
      "Epoch 67: val_loss did not improve from 0.11466\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9551 - loss: 0.1230 - val_accuracy: 0.9615 - val_loss: 0.1222\n",
      "Epoch 68/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9680 - loss: 0.1175\n",
      "Epoch 68: val_loss improved from 0.11466 to 0.11144, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9588 - loss: 0.1246 - val_accuracy: 0.9569 - val_loss: 0.1114\n",
      "Epoch 69/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1196\n",
      "Epoch 69: val_loss improved from 0.11144 to 0.11138, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9606 - loss: 0.1175 - val_accuracy: 0.9577 - val_loss: 0.1114\n",
      "Epoch 70/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9560 - loss: 0.1313\n",
      "Epoch 70: val_loss improved from 0.11138 to 0.11060, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9563 - loss: 0.1260 - val_accuracy: 0.9577 - val_loss: 0.1106\n",
      "Epoch 71/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9380 - loss: 0.1424\n",
      "Epoch 71: val_loss improved from 0.11060 to 0.10968, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9528 - loss: 0.1253 - val_accuracy: 0.9615 - val_loss: 0.1097\n",
      "Epoch 72/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9620 - loss: 0.1127\n",
      "Epoch 72: val_loss improved from 0.10968 to 0.10948, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9599 - loss: 0.1129 - val_accuracy: 0.9608 - val_loss: 0.1095\n",
      "Epoch 73/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9540 - loss: 0.1455\n",
      "Epoch 73: val_loss improved from 0.10948 to 0.10841, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9597 - loss: 0.1227 - val_accuracy: 0.9585 - val_loss: 0.1084\n",
      "Epoch 74/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9560 - loss: 0.1210\n",
      "Epoch 74: val_loss improved from 0.10841 to 0.10788, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9596 - loss: 0.1145 - val_accuracy: 0.9585 - val_loss: 0.1079\n",
      "Epoch 75/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9660 - loss: 0.0995\n",
      "Epoch 75: val_loss improved from 0.10788 to 0.10729, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9631 - loss: 0.1164 - val_accuracy: 0.9592 - val_loss: 0.1073\n",
      "Epoch 76/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9720 - loss: 0.1027\n",
      "Epoch 76: val_loss did not improve from 0.10729\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9647 - loss: 0.1106 - val_accuracy: 0.9577 - val_loss: 0.1082\n",
      "Epoch 77/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1009\n",
      "Epoch 77: val_loss improved from 0.10729 to 0.10520, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9573 - loss: 0.1162 - val_accuracy: 0.9592 - val_loss: 0.1052\n",
      "Epoch 78/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.0966\n",
      "Epoch 78: val_loss improved from 0.10520 to 0.10413, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9599 - loss: 0.1176 - val_accuracy: 0.9600 - val_loss: 0.1041\n",
      "Epoch 79/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9560 - loss: 0.1315\n",
      "Epoch 79: val_loss improved from 0.10413 to 0.10403, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9595 - loss: 0.1187 - val_accuracy: 0.9623 - val_loss: 0.1040\n",
      "Epoch 80/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9680 - loss: 0.1058\n",
      "Epoch 80: val_loss improved from 0.10403 to 0.10251, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9602 - loss: 0.1156 - val_accuracy: 0.9615 - val_loss: 0.1025\n",
      "Epoch 81/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9600 - loss: 0.1428\n",
      "Epoch 81: val_loss did not improve from 0.10251\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9614 - loss: 0.1175 - val_accuracy: 0.9608 - val_loss: 0.1026\n",
      "Epoch 82/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9660 - loss: 0.1264\n",
      "Epoch 82: val_loss improved from 0.10251 to 0.10234, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9648 - loss: 0.1130 - val_accuracy: 0.9646 - val_loss: 0.1023\n",
      "Epoch 83/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9700 - loss: 0.1043\n",
      "Epoch 83: val_loss did not improve from 0.10234\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9629 - loss: 0.1106 - val_accuracy: 0.9638 - val_loss: 0.1060\n",
      "Epoch 84/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9600 - loss: 0.1371\n",
      "Epoch 84: val_loss improved from 0.10234 to 0.10070, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9626 - loss: 0.1176 - val_accuracy: 0.9608 - val_loss: 0.1007\n",
      "Epoch 85/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9740 - loss: 0.0888\n",
      "Epoch 85: val_loss improved from 0.10070 to 0.10036, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9671 - loss: 0.1059 - val_accuracy: 0.9615 - val_loss: 0.1004\n",
      "Epoch 86/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9540 - loss: 0.1208\n",
      "Epoch 86: val_loss improved from 0.10036 to 0.09876, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9640 - loss: 0.1061 - val_accuracy: 0.9615 - val_loss: 0.0988\n",
      "Epoch 87/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9780 - loss: 0.0819\n",
      "Epoch 87: val_loss did not improve from 0.09876\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9681 - loss: 0.1034 - val_accuracy: 0.9646 - val_loss: 0.0990\n",
      "Epoch 88/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9520 - loss: 0.1297\n",
      "Epoch 88: val_loss improved from 0.09876 to 0.09760, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9632 - loss: 0.1110 - val_accuracy: 0.9677 - val_loss: 0.0976\n",
      "Epoch 89/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9680 - loss: 0.1014\n",
      "Epoch 89: val_loss improved from 0.09760 to 0.09696, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9646 - loss: 0.1099 - val_accuracy: 0.9623 - val_loss: 0.0970\n",
      "Epoch 90/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9540 - loss: 0.1302\n",
      "Epoch 90: val_loss did not improve from 0.09696\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9618 - loss: 0.1155 - val_accuracy: 0.9615 - val_loss: 0.1008\n",
      "Epoch 91/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9600 - loss: 0.1043\n",
      "Epoch 91: val_loss improved from 0.09696 to 0.09545, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9627 - loss: 0.1063 - val_accuracy: 0.9662 - val_loss: 0.0954\n",
      "Epoch 92/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9760 - loss: 0.0851\n",
      "Epoch 92: val_loss did not improve from 0.09545\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9649 - loss: 0.1030 - val_accuracy: 0.9669 - val_loss: 0.0963\n",
      "Epoch 93/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9660 - loss: 0.1159\n",
      "Epoch 93: val_loss did not improve from 0.09545\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9633 - loss: 0.1087 - val_accuracy: 0.9685 - val_loss: 0.0980\n",
      "Epoch 94/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9680 - loss: 0.1093\n",
      "Epoch 94: val_loss did not improve from 0.09545\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9635 - loss: 0.1058 - val_accuracy: 0.9669 - val_loss: 0.1033\n",
      "Epoch 95/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9680 - loss: 0.1017\n",
      "Epoch 95: val_loss improved from 0.09545 to 0.09471, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9651 - loss: 0.1104 - val_accuracy: 0.9631 - val_loss: 0.0947\n",
      "Epoch 96/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9640 - loss: 0.1088\n",
      "Epoch 96: val_loss improved from 0.09471 to 0.09262, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9663 - loss: 0.1027 - val_accuracy: 0.9638 - val_loss: 0.0926\n",
      "Epoch 97/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9620 - loss: 0.1121\n",
      "Epoch 97: val_loss improved from 0.09262 to 0.09243, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9654 - loss: 0.1072 - val_accuracy: 0.9654 - val_loss: 0.0924\n",
      "Epoch 98/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9780 - loss: 0.0777\n",
      "Epoch 98: val_loss improved from 0.09243 to 0.09126, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9676 - loss: 0.0994 - val_accuracy: 0.9692 - val_loss: 0.0913\n",
      "Epoch 99/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9800 - loss: 0.0812\n",
      "Epoch 99: val_loss did not improve from 0.09126\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9693 - loss: 0.0993 - val_accuracy: 0.9708 - val_loss: 0.0921\n",
      "Epoch 100/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.0888\n",
      "Epoch 100: val_loss improved from 0.09126 to 0.09109, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9692 - loss: 0.0972 - val_accuracy: 0.9700 - val_loss: 0.0911\n",
      "Epoch 101/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9680 - loss: 0.0951\n",
      "Epoch 101: val_loss improved from 0.09109 to 0.08964, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9696 - loss: 0.0964 - val_accuracy: 0.9685 - val_loss: 0.0896\n",
      "Epoch 102/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9680 - loss: 0.1087\n",
      "Epoch 102: val_loss did not improve from 0.08964\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9677 - loss: 0.1069 - val_accuracy: 0.9631 - val_loss: 0.0934\n",
      "Epoch 103/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9460 - loss: 0.1219\n",
      "Epoch 103: val_loss did not improve from 0.08964\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9649 - loss: 0.1072 - val_accuracy: 0.9615 - val_loss: 0.1021\n",
      "Epoch 104/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9580 - loss: 0.0904\n",
      "Epoch 104: val_loss improved from 0.08964 to 0.08909, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9629 - loss: 0.0987 - val_accuracy: 0.9638 - val_loss: 0.0891\n",
      "Epoch 105/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9540 - loss: 0.1159\n",
      "Epoch 105: val_loss improved from 0.08909 to 0.08749, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9631 - loss: 0.1026 - val_accuracy: 0.9708 - val_loss: 0.0875\n",
      "Epoch 106/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.0996\n",
      "Epoch 106: val_loss did not improve from 0.08749\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9703 - loss: 0.1008 - val_accuracy: 0.9646 - val_loss: 0.0885\n",
      "Epoch 107/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9580 - loss: 0.1233\n",
      "Epoch 107: val_loss improved from 0.08749 to 0.08748, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9671 - loss: 0.1024 - val_accuracy: 0.9654 - val_loss: 0.0875\n",
      "Epoch 108/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9760 - loss: 0.0703\n",
      "Epoch 108: val_loss improved from 0.08748 to 0.08580, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9695 - loss: 0.0935 - val_accuracy: 0.9708 - val_loss: 0.0858\n",
      "Epoch 109/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9620 - loss: 0.0973\n",
      "Epoch 109: val_loss did not improve from 0.08580\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9681 - loss: 0.0964 - val_accuracy: 0.9738 - val_loss: 0.0859\n",
      "Epoch 110/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9680 - loss: 0.1139\n",
      "Epoch 110: val_loss did not improve from 0.08580\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9669 - loss: 0.1010 - val_accuracy: 0.9731 - val_loss: 0.0876\n",
      "Epoch 111/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9720 - loss: 0.0951\n",
      "Epoch 111: val_loss improved from 0.08580 to 0.08501, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9684 - loss: 0.1001 - val_accuracy: 0.9738 - val_loss: 0.0850\n",
      "Epoch 112/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9800 - loss: 0.0729\n",
      "Epoch 112: val_loss did not improve from 0.08501\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9723 - loss: 0.0955 - val_accuracy: 0.9646 - val_loss: 0.0879\n",
      "Epoch 113/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9620 - loss: 0.1367\n",
      "Epoch 113: val_loss did not improve from 0.08501\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.1053 - val_accuracy: 0.9654 - val_loss: 0.0859\n",
      "Epoch 114/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9740 - loss: 0.0807\n",
      "Epoch 114: val_loss improved from 0.08501 to 0.08263, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9689 - loss: 0.0921 - val_accuracy: 0.9731 - val_loss: 0.0826\n",
      "Epoch 115/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9700 - loss: 0.0886\n",
      "Epoch 115: val_loss improved from 0.08263 to 0.08258, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9709 - loss: 0.0885 - val_accuracy: 0.9715 - val_loss: 0.0826\n",
      "Epoch 116/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9580 - loss: 0.1150\n",
      "Epoch 116: val_loss did not improve from 0.08258\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9689 - loss: 0.1011 - val_accuracy: 0.9700 - val_loss: 0.0829\n",
      "Epoch 117/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9680 - loss: 0.0838\n",
      "Epoch 117: val_loss did not improve from 0.08258\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9738 - loss: 0.0874 - val_accuracy: 0.9677 - val_loss: 0.0831\n",
      "Epoch 118/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9620 - loss: 0.1065\n",
      "Epoch 118: val_loss did not improve from 0.08258\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9700 - loss: 0.0986 - val_accuracy: 0.9638 - val_loss: 0.0871\n",
      "Epoch 119/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9660 - loss: 0.1013\n",
      "Epoch 119: val_loss improved from 0.08258 to 0.08156, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9659 - loss: 0.0979 - val_accuracy: 0.9762 - val_loss: 0.0816\n",
      "Epoch 120/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9700 - loss: 0.0803\n",
      "Epoch 120: val_loss improved from 0.08156 to 0.08022, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9733 - loss: 0.0865 - val_accuracy: 0.9731 - val_loss: 0.0802\n",
      "Epoch 121/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9820 - loss: 0.0851\n",
      "Epoch 121: val_loss did not improve from 0.08022\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9753 - loss: 0.0875 - val_accuracy: 0.9723 - val_loss: 0.0821\n",
      "Epoch 122/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9740 - loss: 0.1008\n",
      "Epoch 122: val_loss improved from 0.08022 to 0.08006, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9737 - loss: 0.0878 - val_accuracy: 0.9762 - val_loss: 0.0801\n",
      "Epoch 123/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9660 - loss: 0.1060\n",
      "Epoch 123: val_loss improved from 0.08006 to 0.07859, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9711 - loss: 0.0945 - val_accuracy: 0.9746 - val_loss: 0.0786\n",
      "Epoch 124/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0805\n",
      "Epoch 124: val_loss did not improve from 0.07859\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9789 - loss: 0.0848 - val_accuracy: 0.9754 - val_loss: 0.0790\n",
      "Epoch 125/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9760 - loss: 0.0945\n",
      "Epoch 125: val_loss improved from 0.07859 to 0.07823, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9726 - loss: 0.0917 - val_accuracy: 0.9762 - val_loss: 0.0782\n",
      "Epoch 126/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9780 - loss: 0.0862\n",
      "Epoch 126: val_loss improved from 0.07823 to 0.07724, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9740 - loss: 0.0889 - val_accuracy: 0.9754 - val_loss: 0.0772\n",
      "Epoch 127/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9680 - loss: 0.0979\n",
      "Epoch 127: val_loss did not improve from 0.07724\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9726 - loss: 0.0871 - val_accuracy: 0.9731 - val_loss: 0.0789\n",
      "Epoch 128/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9640 - loss: 0.1126\n",
      "Epoch 128: val_loss improved from 0.07724 to 0.07669, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9674 - loss: 0.1015 - val_accuracy: 0.9746 - val_loss: 0.0767\n",
      "Epoch 129/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9660 - loss: 0.1104\n",
      "Epoch 129: val_loss did not improve from 0.07669\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9711 - loss: 0.0999 - val_accuracy: 0.9662 - val_loss: 0.0823\n",
      "Epoch 130/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.0670\n",
      "Epoch 130: val_loss did not improve from 0.07669\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9716 - loss: 0.0864 - val_accuracy: 0.9669 - val_loss: 0.0820\n",
      "Epoch 131/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9640 - loss: 0.0995\n",
      "Epoch 131: val_loss improved from 0.07669 to 0.07532, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9689 - loss: 0.0957 - val_accuracy: 0.9746 - val_loss: 0.0753\n",
      "Epoch 132/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9720 - loss: 0.0948\n",
      "Epoch 132: val_loss did not improve from 0.07532\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9740 - loss: 0.0865 - val_accuracy: 0.9731 - val_loss: 0.0783\n",
      "Epoch 133/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.0970\n",
      "Epoch 133: val_loss did not improve from 0.07532\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9689 - loss: 0.0979 - val_accuracy: 0.9738 - val_loss: 0.0785\n",
      "Epoch 134/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9680 - loss: 0.0913\n",
      "Epoch 134: val_loss did not improve from 0.07532\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9706 - loss: 0.0863 - val_accuracy: 0.9746 - val_loss: 0.0803\n",
      "Epoch 135/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.0657\n",
      "Epoch 135: val_loss improved from 0.07532 to 0.07411, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9769 - loss: 0.0809 - val_accuracy: 0.9754 - val_loss: 0.0741\n",
      "Epoch 136/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9760 - loss: 0.0697\n",
      "Epoch 136: val_loss did not improve from 0.07411\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9739 - loss: 0.0829 - val_accuracy: 0.9715 - val_loss: 0.0766\n",
      "Epoch 137/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9760 - loss: 0.0798\n",
      "Epoch 137: val_loss did not improve from 0.07411\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9744 - loss: 0.0844 - val_accuracy: 0.9754 - val_loss: 0.0746\n",
      "Epoch 138/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9680 - loss: 0.0996\n",
      "Epoch 138: val_loss improved from 0.07411 to 0.07326, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9708 - loss: 0.0895 - val_accuracy: 0.9754 - val_loss: 0.0733\n",
      "Epoch 139/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9740 - loss: 0.0683\n",
      "Epoch 139: val_loss did not improve from 0.07326\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9741 - loss: 0.0836 - val_accuracy: 0.9731 - val_loss: 0.0753\n",
      "Epoch 140/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9760 - loss: 0.0738\n",
      "Epoch 140: val_loss improved from 0.07326 to 0.07256, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9738 - loss: 0.0822 - val_accuracy: 0.9754 - val_loss: 0.0726\n",
      "Epoch 141/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0839\n",
      "Epoch 141: val_loss improved from 0.07256 to 0.07144, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9765 - loss: 0.0852 - val_accuracy: 0.9754 - val_loss: 0.0714\n",
      "Epoch 142/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0616\n",
      "Epoch 142: val_loss did not improve from 0.07144\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9736 - loss: 0.0851 - val_accuracy: 0.9762 - val_loss: 0.0717\n",
      "Epoch 143/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0701\n",
      "Epoch 143: val_loss did not improve from 0.07144\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9765 - loss: 0.0758 - val_accuracy: 0.9738 - val_loss: 0.0725\n",
      "Epoch 144/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9740 - loss: 0.0944\n",
      "Epoch 144: val_loss improved from 0.07144 to 0.07113, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9743 - loss: 0.0876 - val_accuracy: 0.9746 - val_loss: 0.0711\n",
      "Epoch 145/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9860 - loss: 0.0594\n",
      "Epoch 145: val_loss improved from 0.07113 to 0.07082, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9784 - loss: 0.0759 - val_accuracy: 0.9762 - val_loss: 0.0708\n",
      "Epoch 146/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9760 - loss: 0.0767\n",
      "Epoch 146: val_loss did not improve from 0.07082\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9746 - loss: 0.0834 - val_accuracy: 0.9769 - val_loss: 0.0712\n",
      "Epoch 147/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9680 - loss: 0.0898\n",
      "Epoch 147: val_loss improved from 0.07082 to 0.07007, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9748 - loss: 0.0791 - val_accuracy: 0.9754 - val_loss: 0.0701\n",
      "Epoch 148/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9760 - loss: 0.0882\n",
      "Epoch 148: val_loss did not improve from 0.07007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9743 - loss: 0.0844 - val_accuracy: 0.9777 - val_loss: 0.0704\n",
      "Epoch 149/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9840 - loss: 0.0521\n",
      "Epoch 149: val_loss improved from 0.07007 to 0.06942, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9787 - loss: 0.0751 - val_accuracy: 0.9777 - val_loss: 0.0694\n",
      "Epoch 150/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.0894\n",
      "Epoch 150: val_loss improved from 0.06942 to 0.06877, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9741 - loss: 0.0810 - val_accuracy: 0.9754 - val_loss: 0.0688\n",
      "Epoch 151/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9660 - loss: 0.0968\n",
      "Epoch 151: val_loss did not improve from 0.06877\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9722 - loss: 0.0855 - val_accuracy: 0.9754 - val_loss: 0.0690\n",
      "Epoch 152/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9860 - loss: 0.0558\n",
      "Epoch 152: val_loss improved from 0.06877 to 0.06791, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9773 - loss: 0.0749 - val_accuracy: 0.9769 - val_loss: 0.0679\n",
      "Epoch 153/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9780 - loss: 0.0835\n",
      "Epoch 153: val_loss improved from 0.06791 to 0.06771, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9766 - loss: 0.0789 - val_accuracy: 0.9769 - val_loss: 0.0677\n",
      "Epoch 154/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9740 - loss: 0.0986\n",
      "Epoch 154: val_loss did not improve from 0.06771\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.0842 - val_accuracy: 0.9769 - val_loss: 0.0677\n",
      "Epoch 155/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9760 - loss: 0.0951\n",
      "Epoch 155: val_loss did not improve from 0.06771\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0858 - val_accuracy: 0.9762 - val_loss: 0.0702\n",
      "Epoch 156/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9680 - loss: 0.0958\n",
      "Epoch 156: val_loss did not improve from 0.06771\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9729 - loss: 0.0892 - val_accuracy: 0.9777 - val_loss: 0.0678\n",
      "Epoch 157/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9760 - loss: 0.0695\n",
      "Epoch 157: val_loss did not improve from 0.06771\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0732 - val_accuracy: 0.9754 - val_loss: 0.0685\n",
      "Epoch 158/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9780 - loss: 0.0914\n",
      "Epoch 158: val_loss improved from 0.06771 to 0.06697, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9747 - loss: 0.0806 - val_accuracy: 0.9785 - val_loss: 0.0670\n",
      "Epoch 159/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9740 - loss: 0.0693\n",
      "Epoch 159: val_loss did not improve from 0.06697\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9742 - loss: 0.0805 - val_accuracy: 0.9738 - val_loss: 0.0737\n",
      "Epoch 160/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.0497\n",
      "Epoch 160: val_loss did not improve from 0.06697\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0811 - val_accuracy: 0.9669 - val_loss: 0.0853\n",
      "Epoch 161/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9700 - loss: 0.0819\n",
      "Epoch 161: val_loss did not improve from 0.06697\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9695 - loss: 0.0897 - val_accuracy: 0.9692 - val_loss: 0.0788\n",
      "Epoch 162/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9720 - loss: 0.0918\n",
      "Epoch 162: val_loss did not improve from 0.06697\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9706 - loss: 0.0965 - val_accuracy: 0.9754 - val_loss: 0.0695\n",
      "Epoch 163/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9760 - loss: 0.0786\n",
      "Epoch 163: val_loss improved from 0.06697 to 0.06461, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9722 - loss: 0.0903 - val_accuracy: 0.9777 - val_loss: 0.0646\n",
      "Epoch 164/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9780 - loss: 0.0629\n",
      "Epoch 164: val_loss did not improve from 0.06461\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9726 - loss: 0.0780 - val_accuracy: 0.9777 - val_loss: 0.0704\n",
      "Epoch 165/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.0697\n",
      "Epoch 165: val_loss did not improve from 0.06461\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9770 - loss: 0.0783 - val_accuracy: 0.9777 - val_loss: 0.0661\n",
      "Epoch 166/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9760 - loss: 0.0651\n",
      "Epoch 166: val_loss improved from 0.06461 to 0.06451, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9751 - loss: 0.0792 - val_accuracy: 0.9792 - val_loss: 0.0645\n",
      "Epoch 167/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9780 - loss: 0.0832\n",
      "Epoch 167: val_loss improved from 0.06451 to 0.06433, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.0761 - val_accuracy: 0.9769 - val_loss: 0.0643\n",
      "Epoch 168/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.0732\n",
      "Epoch 168: val_loss improved from 0.06433 to 0.06403, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9762 - loss: 0.0814 - val_accuracy: 0.9785 - val_loss: 0.0640\n",
      "Epoch 169/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9840 - loss: 0.0510\n",
      "Epoch 169: val_loss improved from 0.06403 to 0.06366, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9776 - loss: 0.0699 - val_accuracy: 0.9785 - val_loss: 0.0637\n",
      "Epoch 170/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9820 - loss: 0.0790\n",
      "Epoch 170: val_loss improved from 0.06366 to 0.06309, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9773 - loss: 0.0768 - val_accuracy: 0.9785 - val_loss: 0.0631\n",
      "Epoch 171/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0434\n",
      "Epoch 171: val_loss did not improve from 0.06309\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9785 - loss: 0.0722 - val_accuracy: 0.9769 - val_loss: 0.0662\n",
      "Epoch 172/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.0977\n",
      "Epoch 172: val_loss did not improve from 0.06309\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9762 - loss: 0.0803 - val_accuracy: 0.9785 - val_loss: 0.0649\n",
      "Epoch 173/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9740 - loss: 0.0747\n",
      "Epoch 173: val_loss did not improve from 0.06309\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9749 - loss: 0.0742 - val_accuracy: 0.9792 - val_loss: 0.0637\n",
      "Epoch 174/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0517\n",
      "Epoch 174: val_loss did not improve from 0.06309\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9766 - loss: 0.0689 - val_accuracy: 0.9792 - val_loss: 0.0675\n",
      "Epoch 175/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9720 - loss: 0.0716\n",
      "Epoch 175: val_loss improved from 0.06309 to 0.06212, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9754 - loss: 0.0748 - val_accuracy: 0.9792 - val_loss: 0.0621\n",
      "Epoch 176/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9660 - loss: 0.1176\n",
      "Epoch 176: val_loss did not improve from 0.06212\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9729 - loss: 0.0877 - val_accuracy: 0.9800 - val_loss: 0.0626\n",
      "Epoch 177/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9700 - loss: 0.0819\n",
      "Epoch 177: val_loss did not improve from 0.06212\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.0721 - val_accuracy: 0.9785 - val_loss: 0.0631\n",
      "Epoch 178/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9780 - loss: 0.0907\n",
      "Epoch 178: val_loss improved from 0.06212 to 0.06127, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9790 - loss: 0.0780 - val_accuracy: 0.9792 - val_loss: 0.0613\n",
      "Epoch 179/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9860 - loss: 0.0450\n",
      "Epoch 179: val_loss did not improve from 0.06127\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9812 - loss: 0.0684 - val_accuracy: 0.9785 - val_loss: 0.0626\n",
      "Epoch 180/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9720 - loss: 0.0664\n",
      "Epoch 180: val_loss did not improve from 0.06127\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9762 - loss: 0.0717 - val_accuracy: 0.9785 - val_loss: 0.0639\n",
      "Epoch 181/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9800 - loss: 0.0769\n",
      "Epoch 181: val_loss did not improve from 0.06127\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9784 - loss: 0.0694 - val_accuracy: 0.9792 - val_loss: 0.0633\n",
      "Epoch 182/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9840 - loss: 0.0780\n",
      "Epoch 182: val_loss improved from 0.06127 to 0.06071, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9780 - loss: 0.0787 - val_accuracy: 0.9800 - val_loss: 0.0607\n",
      "Epoch 183/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9780 - loss: 0.0608\n",
      "Epoch 183: val_loss did not improve from 0.06071\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9762 - loss: 0.0748 - val_accuracy: 0.9800 - val_loss: 0.0608\n",
      "Epoch 184/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.0902\n",
      "Epoch 184: val_loss did not improve from 0.06071\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9759 - loss: 0.0744 - val_accuracy: 0.9792 - val_loss: 0.0625\n",
      "Epoch 185/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9740 - loss: 0.0707\n",
      "Epoch 185: val_loss improved from 0.06071 to 0.06007, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9765 - loss: 0.0741 - val_accuracy: 0.9800 - val_loss: 0.0601\n",
      "Epoch 186/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9740 - loss: 0.0777\n",
      "Epoch 186: val_loss did not improve from 0.06007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9765 - loss: 0.0696 - val_accuracy: 0.9792 - val_loss: 0.0663\n",
      "Epoch 187/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9700 - loss: 0.0836\n",
      "Epoch 187: val_loss did not improve from 0.06007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9743 - loss: 0.0787 - val_accuracy: 0.9792 - val_loss: 0.0657\n",
      "Epoch 188/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0915\n",
      "Epoch 188: val_loss did not improve from 0.06007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9785 - loss: 0.0847 - val_accuracy: 0.9808 - val_loss: 0.0604\n",
      "Epoch 189/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9620 - loss: 0.0971\n",
      "Epoch 189: val_loss did not improve from 0.06007\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9748 - loss: 0.0798 - val_accuracy: 0.9792 - val_loss: 0.0604\n",
      "Epoch 190/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9720 - loss: 0.0684\n",
      "Epoch 190: val_loss improved from 0.06007 to 0.05889, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9748 - loss: 0.0707 - val_accuracy: 0.9815 - val_loss: 0.0589\n",
      "Epoch 191/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9780 - loss: 0.0717\n",
      "Epoch 191: val_loss did not improve from 0.05889\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9779 - loss: 0.0689 - val_accuracy: 0.9800 - val_loss: 0.0600\n",
      "Epoch 192/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9720 - loss: 0.0726\n",
      "Epoch 192: val_loss did not improve from 0.05889\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9763 - loss: 0.0714 - val_accuracy: 0.9800 - val_loss: 0.0592\n",
      "Epoch 193/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9700 - loss: 0.0970\n",
      "Epoch 193: val_loss improved from 0.05889 to 0.05842, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9749 - loss: 0.0820 - val_accuracy: 0.9800 - val_loss: 0.0584\n",
      "Epoch 194/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0705\n",
      "Epoch 194: val_loss did not improve from 0.05842\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9821 - loss: 0.0698 - val_accuracy: 0.9769 - val_loss: 0.0629\n",
      "Epoch 195/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9820 - loss: 0.0470\n",
      "Epoch 195: val_loss did not improve from 0.05842\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9796 - loss: 0.0672 - val_accuracy: 0.9800 - val_loss: 0.0595\n",
      "Epoch 196/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9760 - loss: 0.0984\n",
      "Epoch 196: val_loss did not improve from 0.05842\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9769 - loss: 0.0765 - val_accuracy: 0.9808 - val_loss: 0.0589\n",
      "Epoch 197/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9880 - loss: 0.0369\n",
      "Epoch 197: val_loss did not improve from 0.05842\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9790 - loss: 0.0680 - val_accuracy: 0.9800 - val_loss: 0.0593\n",
      "Epoch 198/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9820 - loss: 0.0690\n",
      "Epoch 198: val_loss improved from 0.05842 to 0.05840, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9803 - loss: 0.0659 - val_accuracy: 0.9800 - val_loss: 0.0584\n",
      "Epoch 199/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9700 - loss: 0.1183\n",
      "Epoch 199: val_loss improved from 0.05840 to 0.05785, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9780 - loss: 0.0769 - val_accuracy: 0.9823 - val_loss: 0.0579\n",
      "Epoch 200/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0522\n",
      "Epoch 200: val_loss improved from 0.05785 to 0.05755, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9800 - loss: 0.0626 - val_accuracy: 0.9815 - val_loss: 0.0575\n",
      "Epoch 201/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0791\n",
      "Epoch 201: val_loss did not improve from 0.05755\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.0678 - val_accuracy: 0.9808 - val_loss: 0.0632\n",
      "Epoch 202/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0796\n",
      "Epoch 202: val_loss improved from 0.05755 to 0.05751, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9786 - loss: 0.0719 - val_accuracy: 0.9815 - val_loss: 0.0575\n",
      "Epoch 203/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9800 - loss: 0.1004\n",
      "Epoch 203: val_loss improved from 0.05751 to 0.05722, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9805 - loss: 0.0759 - val_accuracy: 0.9808 - val_loss: 0.0572\n",
      "Epoch 204/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.1210\n",
      "Epoch 204: val_loss improved from 0.05722 to 0.05645, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9747 - loss: 0.0879 - val_accuracy: 0.9823 - val_loss: 0.0564\n",
      "Epoch 205/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9820 - loss: 0.0533\n",
      "Epoch 205: val_loss did not improve from 0.05645\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.0666 - val_accuracy: 0.9792 - val_loss: 0.0586\n",
      "Epoch 206/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9740 - loss: 0.0621\n",
      "Epoch 206: val_loss did not improve from 0.05645\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9791 - loss: 0.0641 - val_accuracy: 0.9808 - val_loss: 0.0593\n",
      "Epoch 207/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9840 - loss: 0.0706\n",
      "Epoch 207: val_loss did not improve from 0.05645\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9785 - loss: 0.0725 - val_accuracy: 0.9815 - val_loss: 0.0570\n",
      "Epoch 208/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9800 - loss: 0.0606\n",
      "Epoch 208: val_loss improved from 0.05645 to 0.05571, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9789 - loss: 0.0710 - val_accuracy: 0.9808 - val_loss: 0.0557\n",
      "Epoch 209/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9720 - loss: 0.0792\n",
      "Epoch 209: val_loss did not improve from 0.05571\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0729 - val_accuracy: 0.9792 - val_loss: 0.0582\n",
      "Epoch 210/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0724\n",
      "Epoch 210: val_loss did not improve from 0.05571\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9797 - loss: 0.0739 - val_accuracy: 0.9762 - val_loss: 0.0646\n",
      "Epoch 211/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.0962\n",
      "Epoch 211: val_loss improved from 0.05571 to 0.05498, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9781 - loss: 0.0735 - val_accuracy: 0.9800 - val_loss: 0.0550\n",
      "Epoch 212/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9840 - loss: 0.0411\n",
      "Epoch 212: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9793 - loss: 0.0661 - val_accuracy: 0.9815 - val_loss: 0.0567\n",
      "Epoch 213/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0531\n",
      "Epoch 213: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9800 - loss: 0.0674 - val_accuracy: 0.9808 - val_loss: 0.0554\n",
      "Epoch 214/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9760 - loss: 0.0855\n",
      "Epoch 214: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9788 - loss: 0.0702 - val_accuracy: 0.9808 - val_loss: 0.0552\n",
      "Epoch 215/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9760 - loss: 0.1037\n",
      "Epoch 215: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9780 - loss: 0.0799 - val_accuracy: 0.9762 - val_loss: 0.0683\n",
      "Epoch 216/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9840 - loss: 0.0421\n",
      "Epoch 216: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9803 - loss: 0.0644 - val_accuracy: 0.9808 - val_loss: 0.0561\n",
      "Epoch 217/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9660 - loss: 0.1068\n",
      "Epoch 217: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9764 - loss: 0.0781 - val_accuracy: 0.9800 - val_loss: 0.0597\n",
      "Epoch 218/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0591\n",
      "Epoch 218: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9792 - loss: 0.0721 - val_accuracy: 0.9777 - val_loss: 0.0606\n",
      "Epoch 219/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9760 - loss: 0.0633\n",
      "Epoch 219: val_loss did not improve from 0.05498\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.0738 - val_accuracy: 0.9792 - val_loss: 0.0610\n",
      "Epoch 220/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9660 - loss: 0.1065\n",
      "Epoch 220: val_loss improved from 0.05498 to 0.05478, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9743 - loss: 0.0782 - val_accuracy: 0.9823 - val_loss: 0.0548\n",
      "Epoch 221/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9840 - loss: 0.0511\n",
      "Epoch 221: val_loss did not improve from 0.05478\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9810 - loss: 0.0657 - val_accuracy: 0.9808 - val_loss: 0.0551\n",
      "Epoch 222/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0615\n",
      "Epoch 222: val_loss did not improve from 0.05478\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9805 - loss: 0.0633 - val_accuracy: 0.9815 - val_loss: 0.0554\n",
      "Epoch 223/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9700 - loss: 0.0721\n",
      "Epoch 223: val_loss did not improve from 0.05478\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9774 - loss: 0.0721 - val_accuracy: 0.9800 - val_loss: 0.0599\n",
      "Epoch 224/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9780 - loss: 0.0619\n",
      "Epoch 224: val_loss improved from 0.05478 to 0.05464, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9822 - loss: 0.0670 - val_accuracy: 0.9815 - val_loss: 0.0546\n",
      "Epoch 225/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9780 - loss: 0.0728\n",
      "Epoch 225: val_loss improved from 0.05464 to 0.05423, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9782 - loss: 0.0688 - val_accuracy: 0.9815 - val_loss: 0.0542\n",
      "Epoch 226/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0501\n",
      "Epoch 226: val_loss improved from 0.05423 to 0.05379, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9785 - loss: 0.0692 - val_accuracy: 0.9815 - val_loss: 0.0538\n",
      "Epoch 227/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9860 - loss: 0.0712\n",
      "Epoch 227: val_loss did not improve from 0.05379\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9834 - loss: 0.0661 - val_accuracy: 0.9815 - val_loss: 0.0604\n",
      "Epoch 228/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0642\n",
      "Epoch 228: val_loss did not improve from 0.05379\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9817 - loss: 0.0679 - val_accuracy: 0.9815 - val_loss: 0.0541\n",
      "Epoch 229/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9800 - loss: 0.0763\n",
      "Epoch 229: val_loss improved from 0.05379 to 0.05377, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9818 - loss: 0.0679 - val_accuracy: 0.9815 - val_loss: 0.0538\n",
      "Epoch 230/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0712\n",
      "Epoch 230: val_loss did not improve from 0.05377\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9794 - loss: 0.0711 - val_accuracy: 0.9792 - val_loss: 0.0580\n",
      "Epoch 231/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9700 - loss: 0.1055\n",
      "Epoch 231: val_loss did not improve from 0.05377\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9788 - loss: 0.0760 - val_accuracy: 0.9792 - val_loss: 0.0561\n",
      "Epoch 232/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9880 - loss: 0.0406\n",
      "Epoch 232: val_loss did not improve from 0.05377\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9800 - loss: 0.0618 - val_accuracy: 0.9792 - val_loss: 0.0577\n",
      "Epoch 233/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0572\n",
      "Epoch 233: val_loss did not improve from 0.05377\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9791 - loss: 0.0630 - val_accuracy: 0.9815 - val_loss: 0.0540\n",
      "Epoch 234/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9820 - loss: 0.0417\n",
      "Epoch 234: val_loss did not improve from 0.05377\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9806 - loss: 0.0594 - val_accuracy: 0.9815 - val_loss: 0.0545\n",
      "Epoch 235/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9820 - loss: 0.0768\n",
      "Epoch 235: val_loss improved from 0.05377 to 0.05273, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9819 - loss: 0.0673 - val_accuracy: 0.9815 - val_loss: 0.0527\n",
      "Epoch 236/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9800 - loss: 0.0910\n",
      "Epoch 236: val_loss did not improve from 0.05273\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9788 - loss: 0.0764 - val_accuracy: 0.9808 - val_loss: 0.0551\n",
      "Epoch 237/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0559\n",
      "Epoch 237: val_loss did not improve from 0.05273\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9789 - loss: 0.0646 - val_accuracy: 0.9815 - val_loss: 0.0555\n",
      "Epoch 238/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.0777\n",
      "Epoch 238: val_loss improved from 0.05273 to 0.05273, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9823 - loss: 0.0618 - val_accuracy: 0.9815 - val_loss: 0.0527\n",
      "Epoch 239/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9920 - loss: 0.0394\n",
      "Epoch 239: val_loss improved from 0.05273 to 0.05209, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9828 - loss: 0.0615 - val_accuracy: 0.9815 - val_loss: 0.0521\n",
      "Epoch 240/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9760 - loss: 0.0718\n",
      "Epoch 240: val_loss did not improve from 0.05209\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9797 - loss: 0.0642 - val_accuracy: 0.9815 - val_loss: 0.0535\n",
      "Epoch 241/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9860 - loss: 0.0629\n",
      "Epoch 241: val_loss did not improve from 0.05209\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9820 - loss: 0.0628 - val_accuracy: 0.9808 - val_loss: 0.0525\n",
      "Epoch 242/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0516\n",
      "Epoch 242: val_loss did not improve from 0.05209\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9827 - loss: 0.0622 - val_accuracy: 0.9792 - val_loss: 0.0559\n",
      "Epoch 243/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.1046\n",
      "Epoch 243: val_loss did not improve from 0.05209\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.0773 - val_accuracy: 0.9792 - val_loss: 0.0556\n",
      "Epoch 244/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9720 - loss: 0.0800\n",
      "Epoch 244: val_loss improved from 0.05209 to 0.05205, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9809 - loss: 0.0663 - val_accuracy: 0.9815 - val_loss: 0.0521\n",
      "Epoch 245/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0488\n",
      "Epoch 245: val_loss did not improve from 0.05205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9829 - loss: 0.0648 - val_accuracy: 0.9800 - val_loss: 0.0583\n",
      "Epoch 246/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0766\n",
      "Epoch 246: val_loss did not improve from 0.05205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0770 - val_accuracy: 0.9800 - val_loss: 0.0571\n",
      "Epoch 247/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9760 - loss: 0.0604\n",
      "Epoch 247: val_loss did not improve from 0.05205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.0648 - val_accuracy: 0.9800 - val_loss: 0.0539\n",
      "Epoch 248/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0512\n",
      "Epoch 248: val_loss did not improve from 0.05205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9783 - loss: 0.0657 - val_accuracy: 0.9823 - val_loss: 0.0521\n",
      "Epoch 249/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9820 - loss: 0.0604\n",
      "Epoch 249: val_loss did not improve from 0.05205\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9812 - loss: 0.0671 - val_accuracy: 0.9808 - val_loss: 0.0604\n",
      "Epoch 250/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9800 - loss: 0.0767\n",
      "Epoch 250: val_loss improved from 0.05205 to 0.05158, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9794 - loss: 0.0697 - val_accuracy: 0.9823 - val_loss: 0.0516\n",
      "Epoch 251/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9780 - loss: 0.0702\n",
      "Epoch 251: val_loss did not improve from 0.05158\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9812 - loss: 0.0653 - val_accuracy: 0.9800 - val_loss: 0.0550\n",
      "Epoch 252/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9820 - loss: 0.0490\n",
      "Epoch 252: val_loss improved from 0.05158 to 0.05141, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9822 - loss: 0.0562 - val_accuracy: 0.9823 - val_loss: 0.0514\n",
      "Epoch 253/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9840 - loss: 0.0497\n",
      "Epoch 253: val_loss improved from 0.05141 to 0.05140, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.0560 - val_accuracy: 0.9808 - val_loss: 0.0514\n",
      "Epoch 254/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0825\n",
      "Epoch 254: val_loss did not improve from 0.05140\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9813 - loss: 0.0697 - val_accuracy: 0.9808 - val_loss: 0.0514\n",
      "Epoch 255/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0521\n",
      "Epoch 255: val_loss did not improve from 0.05140\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0597 - val_accuracy: 0.9815 - val_loss: 0.0519\n",
      "Epoch 256/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9740 - loss: 0.0618\n",
      "Epoch 256: val_loss did not improve from 0.05140\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.0648 - val_accuracy: 0.9815 - val_loss: 0.0518\n",
      "Epoch 257/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9920 - loss: 0.0579\n",
      "Epoch 257: val_loss improved from 0.05140 to 0.05070, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9870 - loss: 0.0586 - val_accuracy: 0.9815 - val_loss: 0.0507\n",
      "Epoch 258/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9860 - loss: 0.0488\n",
      "Epoch 258: val_loss did not improve from 0.05070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9820 - loss: 0.0631 - val_accuracy: 0.9815 - val_loss: 0.0523\n",
      "Epoch 259/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9860 - loss: 0.0471\n",
      "Epoch 259: val_loss did not improve from 0.05070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0606 - val_accuracy: 0.9831 - val_loss: 0.0516\n",
      "Epoch 260/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9860 - loss: 0.0621\n",
      "Epoch 260: val_loss did not improve from 0.05070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9838 - loss: 0.0617 - val_accuracy: 0.9838 - val_loss: 0.0508\n",
      "Epoch 261/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9700 - loss: 0.1000\n",
      "Epoch 261: val_loss did not improve from 0.05070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9799 - loss: 0.0711 - val_accuracy: 0.9831 - val_loss: 0.0525\n",
      "Epoch 262/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9820 - loss: 0.0736\n",
      "Epoch 262: val_loss improved from 0.05070 to 0.05027, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9837 - loss: 0.0640 - val_accuracy: 0.9815 - val_loss: 0.0503\n",
      "Epoch 263/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.0530\n",
      "Epoch 263: val_loss did not improve from 0.05027\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9844 - loss: 0.0591 - val_accuracy: 0.9815 - val_loss: 0.0508\n",
      "Epoch 264/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9780 - loss: 0.0753\n",
      "Epoch 264: val_loss did not improve from 0.05027\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9822 - loss: 0.0683 - val_accuracy: 0.9823 - val_loss: 0.0504\n",
      "Epoch 265/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0472\n",
      "Epoch 265: val_loss did not improve from 0.05027\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9850 - loss: 0.0560 - val_accuracy: 0.9831 - val_loss: 0.0514\n",
      "Epoch 266/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9760 - loss: 0.0932\n",
      "Epoch 266: val_loss did not improve from 0.05027\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9821 - loss: 0.0680 - val_accuracy: 0.9831 - val_loss: 0.0519\n",
      "Epoch 267/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9760 - loss: 0.0882\n",
      "Epoch 267: val_loss improved from 0.05027 to 0.04897, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9831 - loss: 0.0656 - val_accuracy: 0.9838 - val_loss: 0.0490\n",
      "Epoch 268/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9820 - loss: 0.0660\n",
      "Epoch 268: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9820 - loss: 0.0634 - val_accuracy: 0.9831 - val_loss: 0.0501\n",
      "Epoch 269/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0572\n",
      "Epoch 269: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9812 - loss: 0.0605 - val_accuracy: 0.9823 - val_loss: 0.0495\n",
      "Epoch 270/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0599\n",
      "Epoch 270: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9828 - loss: 0.0607 - val_accuracy: 0.9831 - val_loss: 0.0521\n",
      "Epoch 271/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0665\n",
      "Epoch 271: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.0630 - val_accuracy: 0.9831 - val_loss: 0.0527\n",
      "Epoch 272/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.0539\n",
      "Epoch 272: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.0630 - val_accuracy: 0.9823 - val_loss: 0.0493\n",
      "Epoch 273/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9840 - loss: 0.0767\n",
      "Epoch 273: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9843 - loss: 0.0627 - val_accuracy: 0.9815 - val_loss: 0.0506\n",
      "Epoch 274/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9800 - loss: 0.0856\n",
      "Epoch 274: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9835 - loss: 0.0676 - val_accuracy: 0.9831 - val_loss: 0.0493\n",
      "Epoch 275/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9840 - loss: 0.0476\n",
      "Epoch 275: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9835 - loss: 0.0586 - val_accuracy: 0.9823 - val_loss: 0.0491\n",
      "Epoch 276/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0689\n",
      "Epoch 276: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9839 - loss: 0.0629 - val_accuracy: 0.9831 - val_loss: 0.0516\n",
      "Epoch 277/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9780 - loss: 0.0805\n",
      "Epoch 277: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9835 - loss: 0.0650 - val_accuracy: 0.9823 - val_loss: 0.0505\n",
      "Epoch 278/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0607\n",
      "Epoch 278: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.0644 - val_accuracy: 0.9831 - val_loss: 0.0493\n",
      "Epoch 279/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0417\n",
      "Epoch 279: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9803 - loss: 0.0648 - val_accuracy: 0.9800 - val_loss: 0.0591\n",
      "Epoch 280/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9680 - loss: 0.0811\n",
      "Epoch 280: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9764 - loss: 0.0727 - val_accuracy: 0.9800 - val_loss: 0.0549\n",
      "Epoch 281/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9820 - loss: 0.0572\n",
      "Epoch 281: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9805 - loss: 0.0713 - val_accuracy: 0.9815 - val_loss: 0.0563\n",
      "Epoch 282/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0711\n",
      "Epoch 282: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9823 - loss: 0.0672 - val_accuracy: 0.9815 - val_loss: 0.0554\n",
      "Epoch 283/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9780 - loss: 0.0749\n",
      "Epoch 283: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9809 - loss: 0.0691 - val_accuracy: 0.9792 - val_loss: 0.0645\n",
      "Epoch 284/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9840 - loss: 0.0759\n",
      "Epoch 284: val_loss did not improve from 0.04897\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9825 - loss: 0.0678 - val_accuracy: 0.9838 - val_loss: 0.0508\n",
      "Epoch 285/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9900 - loss: 0.0318\n",
      "Epoch 285: val_loss improved from 0.04897 to 0.04840, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9840 - loss: 0.0535 - val_accuracy: 0.9823 - val_loss: 0.0484\n",
      "Epoch 286/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9780 - loss: 0.0919\n",
      "Epoch 286: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9823 - loss: 0.0686 - val_accuracy: 0.9831 - val_loss: 0.0490\n",
      "Epoch 287/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9840 - loss: 0.0553\n",
      "Epoch 287: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9837 - loss: 0.0580 - val_accuracy: 0.9823 - val_loss: 0.0485\n",
      "Epoch 288/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9920 - loss: 0.0558\n",
      "Epoch 288: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9852 - loss: 0.0573 - val_accuracy: 0.9831 - val_loss: 0.0525\n",
      "Epoch 289/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0505\n",
      "Epoch 289: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9823 - loss: 0.0604 - val_accuracy: 0.9831 - val_loss: 0.0624\n",
      "Epoch 290/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9760 - loss: 0.0737\n",
      "Epoch 290: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.0726 - val_accuracy: 0.9746 - val_loss: 0.0867\n",
      "Epoch 291/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9700 - loss: 0.1251\n",
      "Epoch 291: val_loss did not improve from 0.04840\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9774 - loss: 0.0818 - val_accuracy: 0.9823 - val_loss: 0.0485\n",
      "Epoch 292/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0751\n",
      "Epoch 292: val_loss improved from 0.04840 to 0.04723, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9809 - loss: 0.0674 - val_accuracy: 0.9838 - val_loss: 0.0472\n",
      "Epoch 293/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9820 - loss: 0.0676\n",
      "Epoch 293: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9798 - loss: 0.0714 - val_accuracy: 0.9800 - val_loss: 0.0540\n",
      "Epoch 294/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0461\n",
      "Epoch 294: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0573 - val_accuracy: 0.9831 - val_loss: 0.0486\n",
      "Epoch 295/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0518\n",
      "Epoch 295: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0585 - val_accuracy: 0.9823 - val_loss: 0.0476\n",
      "Epoch 296/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9780 - loss: 0.0505\n",
      "Epoch 296: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9812 - loss: 0.0641 - val_accuracy: 0.9831 - val_loss: 0.0484\n",
      "Epoch 297/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9900 - loss: 0.0504\n",
      "Epoch 297: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9849 - loss: 0.0598 - val_accuracy: 0.9831 - val_loss: 0.0507\n",
      "Epoch 298/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9780 - loss: 0.0513\n",
      "Epoch 298: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9810 - loss: 0.0619 - val_accuracy: 0.9800 - val_loss: 0.0603\n",
      "Epoch 299/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0620\n",
      "Epoch 299: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9809 - loss: 0.0651 - val_accuracy: 0.9838 - val_loss: 0.0496\n",
      "Epoch 300/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9820 - loss: 0.0701\n",
      "Epoch 300: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9851 - loss: 0.0568 - val_accuracy: 0.9831 - val_loss: 0.0500\n",
      "Epoch 301/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9920 - loss: 0.0414\n",
      "Epoch 301: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0575 - val_accuracy: 0.9831 - val_loss: 0.0511\n",
      "Epoch 302/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9880 - loss: 0.0267\n",
      "Epoch 302: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9812 - loss: 0.0593 - val_accuracy: 0.9823 - val_loss: 0.0474\n",
      "Epoch 303/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9960 - loss: 0.0271\n",
      "Epoch 303: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9855 - loss: 0.0574 - val_accuracy: 0.9831 - val_loss: 0.0501\n",
      "Epoch 304/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9880 - loss: 0.0383\n",
      "Epoch 304: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0570 - val_accuracy: 0.9831 - val_loss: 0.0496\n",
      "Epoch 305/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9780 - loss: 0.0746\n",
      "Epoch 305: val_loss did not improve from 0.04723\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9830 - loss: 0.0654 - val_accuracy: 0.9823 - val_loss: 0.0482\n",
      "Epoch 306/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9860 - loss: 0.0507\n",
      "Epoch 306: val_loss improved from 0.04723 to 0.04709, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9847 - loss: 0.0583 - val_accuracy: 0.9846 - val_loss: 0.0471\n",
      "Epoch 307/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9780 - loss: 0.0800\n",
      "Epoch 307: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9824 - loss: 0.0630 - val_accuracy: 0.9831 - val_loss: 0.0498\n",
      "Epoch 308/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0747\n",
      "Epoch 308: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9831 - loss: 0.0580 - val_accuracy: 0.9831 - val_loss: 0.0491\n",
      "Epoch 309/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0607\n",
      "Epoch 309: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.0578 - val_accuracy: 0.9831 - val_loss: 0.0498\n",
      "Epoch 310/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9780 - loss: 0.0670\n",
      "Epoch 310: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9844 - loss: 0.0591 - val_accuracy: 0.9823 - val_loss: 0.0483\n",
      "Epoch 311/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9920 - loss: 0.0427\n",
      "Epoch 311: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9846 - loss: 0.0581 - val_accuracy: 0.9838 - val_loss: 0.0510\n",
      "Epoch 312/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9820 - loss: 0.0838\n",
      "Epoch 312: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9856 - loss: 0.0620 - val_accuracy: 0.9831 - val_loss: 0.0494\n",
      "Epoch 313/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0479\n",
      "Epoch 313: val_loss did not improve from 0.04709\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9846 - loss: 0.0551 - val_accuracy: 0.9831 - val_loss: 0.0484\n",
      "Epoch 314/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.0367\n",
      "Epoch 314: val_loss improved from 0.04709 to 0.04680, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9856 - loss: 0.0536 - val_accuracy: 0.9854 - val_loss: 0.0468\n",
      "Epoch 315/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0468\n",
      "Epoch 315: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0600 - val_accuracy: 0.9823 - val_loss: 0.0491\n",
      "Epoch 316/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0599\n",
      "Epoch 316: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0595 - val_accuracy: 0.9838 - val_loss: 0.0481\n",
      "Epoch 317/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0654\n",
      "Epoch 317: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9849 - loss: 0.0653 - val_accuracy: 0.9823 - val_loss: 0.0503\n",
      "Epoch 318/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.0692\n",
      "Epoch 318: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9845 - loss: 0.0621 - val_accuracy: 0.9823 - val_loss: 0.0493\n",
      "Epoch 319/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.0410\n",
      "Epoch 319: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9852 - loss: 0.0499 - val_accuracy: 0.9831 - val_loss: 0.0478\n",
      "Epoch 320/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9820 - loss: 0.0575\n",
      "Epoch 320: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9845 - loss: 0.0559 - val_accuracy: 0.9815 - val_loss: 0.0484\n",
      "Epoch 321/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0467\n",
      "Epoch 321: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9853 - loss: 0.0534 - val_accuracy: 0.9846 - val_loss: 0.0472\n",
      "Epoch 322/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9860 - loss: 0.0441\n",
      "Epoch 322: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9828 - loss: 0.0548 - val_accuracy: 0.9846 - val_loss: 0.0475\n",
      "Epoch 323/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9720 - loss: 0.0947\n",
      "Epoch 323: val_loss did not improve from 0.04680\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9807 - loss: 0.0661 - val_accuracy: 0.9838 - val_loss: 0.0510\n",
      "Epoch 324/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9820 - loss: 0.0536\n",
      "Epoch 324: val_loss improved from 0.04680 to 0.04659, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9836 - loss: 0.0595 - val_accuracy: 0.9846 - val_loss: 0.0466\n",
      "Epoch 325/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9880 - loss: 0.0431\n",
      "Epoch 325: val_loss improved from 0.04659 to 0.04632, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0478 - val_accuracy: 0.9838 - val_loss: 0.0463\n",
      "Epoch 326/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0508\n",
      "Epoch 326: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9825 - loss: 0.0600 - val_accuracy: 0.9846 - val_loss: 0.0468\n",
      "Epoch 327/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9860 - loss: 0.0529\n",
      "Epoch 327: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9861 - loss: 0.0587 - val_accuracy: 0.9838 - val_loss: 0.0480\n",
      "Epoch 328/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9920 - loss: 0.0368\n",
      "Epoch 328: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9856 - loss: 0.0550 - val_accuracy: 0.9846 - val_loss: 0.0472\n",
      "Epoch 329/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9880 - loss: 0.0475\n",
      "Epoch 329: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.0569 - val_accuracy: 0.9846 - val_loss: 0.0496\n",
      "Epoch 330/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0614\n",
      "Epoch 330: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9839 - loss: 0.0572 - val_accuracy: 0.9846 - val_loss: 0.0468\n",
      "Epoch 331/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0633\n",
      "Epoch 331: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9823 - loss: 0.0621 - val_accuracy: 0.9831 - val_loss: 0.0495\n",
      "Epoch 332/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0593\n",
      "Epoch 332: val_loss did not improve from 0.04632\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9837 - loss: 0.0582 - val_accuracy: 0.9838 - val_loss: 0.0500\n",
      "Epoch 333/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9860 - loss: 0.0509\n",
      "Epoch 333: val_loss improved from 0.04632 to 0.04611, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9844 - loss: 0.0594 - val_accuracy: 0.9854 - val_loss: 0.0461\n",
      "Epoch 334/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.0659\n",
      "Epoch 334: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0623 - val_accuracy: 0.9831 - val_loss: 0.0483\n",
      "Epoch 335/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9940 - loss: 0.0393\n",
      "Epoch 335: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9872 - loss: 0.0543 - val_accuracy: 0.9838 - val_loss: 0.0478\n",
      "Epoch 336/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0613\n",
      "Epoch 336: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9839 - loss: 0.0614 - val_accuracy: 0.9838 - val_loss: 0.0493\n",
      "Epoch 337/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0501\n",
      "Epoch 337: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0592 - val_accuracy: 0.9846 - val_loss: 0.0467\n",
      "Epoch 338/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0954\n",
      "Epoch 338: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9840 - loss: 0.0679 - val_accuracy: 0.9838 - val_loss: 0.0510\n",
      "Epoch 339/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9880 - loss: 0.0383\n",
      "Epoch 339: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9856 - loss: 0.0525 - val_accuracy: 0.9831 - val_loss: 0.0506\n",
      "Epoch 340/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9860 - loss: 0.0397\n",
      "Epoch 340: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9839 - loss: 0.0500 - val_accuracy: 0.9854 - val_loss: 0.0463\n",
      "Epoch 341/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9880 - loss: 0.0481\n",
      "Epoch 341: val_loss did not improve from 0.04611\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.0538 - val_accuracy: 0.9846 - val_loss: 0.0472\n",
      "Epoch 342/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9840 - loss: 0.0569\n",
      "Epoch 342: val_loss improved from 0.04611 to 0.04610, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9847 - loss: 0.0531 - val_accuracy: 0.9838 - val_loss: 0.0461\n",
      "Epoch 343/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9880 - loss: 0.0396\n",
      "Epoch 343: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9869 - loss: 0.0511 - val_accuracy: 0.9846 - val_loss: 0.0474\n",
      "Epoch 344/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9840 - loss: 0.0700\n",
      "Epoch 344: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0593 - val_accuracy: 0.9838 - val_loss: 0.0472\n",
      "Epoch 345/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9960 - loss: 0.0287\n",
      "Epoch 345: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9876 - loss: 0.0488 - val_accuracy: 0.9831 - val_loss: 0.0472\n",
      "Epoch 346/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9840 - loss: 0.0559\n",
      "Epoch 346: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9851 - loss: 0.0569 - val_accuracy: 0.9823 - val_loss: 0.0478\n",
      "Epoch 347/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9860 - loss: 0.0566\n",
      "Epoch 347: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9850 - loss: 0.0599 - val_accuracy: 0.9846 - val_loss: 0.0465\n",
      "Epoch 348/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9900 - loss: 0.0452\n",
      "Epoch 348: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9862 - loss: 0.0519 - val_accuracy: 0.9854 - val_loss: 0.0476\n",
      "Epoch 349/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0431\n",
      "Epoch 349: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.0496 - val_accuracy: 0.9846 - val_loss: 0.0465\n",
      "Epoch 350/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9840 - loss: 0.0585\n",
      "Epoch 350: val_loss did not improve from 0.04610\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9846 - loss: 0.0564 - val_accuracy: 0.9815 - val_loss: 0.0471\n",
      "Epoch 351/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0490\n",
      "Epoch 351: val_loss improved from 0.04610 to 0.04562, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0557 - val_accuracy: 0.9854 - val_loss: 0.0456\n",
      "Epoch 352/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0638\n",
      "Epoch 352: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9843 - loss: 0.0580 - val_accuracy: 0.9846 - val_loss: 0.0465\n",
      "Epoch 353/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0600\n",
      "Epoch 353: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9847 - loss: 0.0607 - val_accuracy: 0.9846 - val_loss: 0.0460\n",
      "Epoch 354/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0514\n",
      "Epoch 354: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9851 - loss: 0.0573 - val_accuracy: 0.9846 - val_loss: 0.0459\n",
      "Epoch 355/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9840 - loss: 0.0513\n",
      "Epoch 355: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0535 - val_accuracy: 0.9846 - val_loss: 0.0464\n",
      "Epoch 356/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0637\n",
      "Epoch 356: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0588 - val_accuracy: 0.9831 - val_loss: 0.0552\n",
      "Epoch 357/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0548\n",
      "Epoch 357: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9829 - loss: 0.0607 - val_accuracy: 0.9831 - val_loss: 0.0569\n",
      "Epoch 358/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0788\n",
      "Epoch 358: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9822 - loss: 0.0647 - val_accuracy: 0.9838 - val_loss: 0.0507\n",
      "Epoch 359/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9860 - loss: 0.0768\n",
      "Epoch 359: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9825 - loss: 0.0672 - val_accuracy: 0.9846 - val_loss: 0.0483\n",
      "Epoch 360/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.0837\n",
      "Epoch 360: val_loss did not improve from 0.04562\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9847 - loss: 0.0634 - val_accuracy: 0.9831 - val_loss: 0.0474\n",
      "Epoch 361/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9780 - loss: 0.0670\n",
      "Epoch 361: val_loss improved from 0.04562 to 0.04537, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9851 - loss: 0.0556 - val_accuracy: 0.9846 - val_loss: 0.0454\n",
      "Epoch 362/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9840 - loss: 0.0432\n",
      "Epoch 362: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.0513 - val_accuracy: 0.9838 - val_loss: 0.0464\n",
      "Epoch 363/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9860 - loss: 0.0919\n",
      "Epoch 363: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9851 - loss: 0.0650 - val_accuracy: 0.9815 - val_loss: 0.0470\n",
      "Epoch 364/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9960 - loss: 0.0196\n",
      "Epoch 364: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9855 - loss: 0.0484 - val_accuracy: 0.9846 - val_loss: 0.0499\n",
      "Epoch 365/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9860 - loss: 0.0486\n",
      "Epoch 365: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9840 - loss: 0.0542 - val_accuracy: 0.9838 - val_loss: 0.0457\n",
      "Epoch 366/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0674\n",
      "Epoch 366: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0590 - val_accuracy: 0.9854 - val_loss: 0.0467\n",
      "Epoch 367/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9920 - loss: 0.0477\n",
      "Epoch 367: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9872 - loss: 0.0510 - val_accuracy: 0.9823 - val_loss: 0.0562\n",
      "Epoch 368/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9740 - loss: 0.1019\n",
      "Epoch 368: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9815 - loss: 0.0701 - val_accuracy: 0.9846 - val_loss: 0.0465\n",
      "Epoch 369/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9820 - loss: 0.0467\n",
      "Epoch 369: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0510 - val_accuracy: 0.9846 - val_loss: 0.0462\n",
      "Epoch 370/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0610\n",
      "Epoch 370: val_loss did not improve from 0.04537\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0572 - val_accuracy: 0.9838 - val_loss: 0.0461\n",
      "Epoch 371/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9900 - loss: 0.0352\n",
      "Epoch 371: val_loss improved from 0.04537 to 0.04535, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9855 - loss: 0.0510 - val_accuracy: 0.9846 - val_loss: 0.0453\n",
      "Epoch 372/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9920 - loss: 0.0317\n",
      "Epoch 372: val_loss did not improve from 0.04535\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9875 - loss: 0.0487 - val_accuracy: 0.9846 - val_loss: 0.0470\n",
      "Epoch 373/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9860 - loss: 0.0660\n",
      "Epoch 373: val_loss did not improve from 0.04535\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9864 - loss: 0.0556 - val_accuracy: 0.9831 - val_loss: 0.0470\n",
      "Epoch 374/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9820 - loss: 0.0987\n",
      "Epoch 374: val_loss did not improve from 0.04535\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9815 - loss: 0.0706 - val_accuracy: 0.9854 - val_loss: 0.0463\n",
      "Epoch 375/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9880 - loss: 0.0454\n",
      "Epoch 375: val_loss did not improve from 0.04535\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9850 - loss: 0.0603 - val_accuracy: 0.9838 - val_loss: 0.0485\n",
      "Epoch 376/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9860 - loss: 0.0565\n",
      "Epoch 376: val_loss did not improve from 0.04535\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9851 - loss: 0.0546 - val_accuracy: 0.9846 - val_loss: 0.0462\n",
      "Epoch 377/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9920 - loss: 0.0267\n",
      "Epoch 377: val_loss improved from 0.04535 to 0.04483, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9864 - loss: 0.0511 - val_accuracy: 0.9854 - val_loss: 0.0448\n",
      "Epoch 378/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9800 - loss: 0.0626\n",
      "Epoch 378: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9820 - loss: 0.0662 - val_accuracy: 0.9838 - val_loss: 0.0492\n",
      "Epoch 379/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0572\n",
      "Epoch 379: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0534 - val_accuracy: 0.9846 - val_loss: 0.0481\n",
      "Epoch 380/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9880 - loss: 0.0601\n",
      "Epoch 380: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9862 - loss: 0.0557 - val_accuracy: 0.9846 - val_loss: 0.0457\n",
      "Epoch 381/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0579\n",
      "Epoch 381: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9845 - loss: 0.0536 - val_accuracy: 0.9854 - val_loss: 0.0455\n",
      "Epoch 382/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0540\n",
      "Epoch 382: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.0593 - val_accuracy: 0.9846 - val_loss: 0.0502\n",
      "Epoch 383/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.0444\n",
      "Epoch 383: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9857 - loss: 0.0529 - val_accuracy: 0.9815 - val_loss: 0.0555\n",
      "Epoch 384/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0748\n",
      "Epoch 384: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9847 - loss: 0.0635 - val_accuracy: 0.9823 - val_loss: 0.0507\n",
      "Epoch 385/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9860 - loss: 0.0419\n",
      "Epoch 385: val_loss did not improve from 0.04483\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9850 - loss: 0.0527 - val_accuracy: 0.9838 - val_loss: 0.0461\n",
      "Epoch 386/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9800 - loss: 0.0467\n",
      "Epoch 386: val_loss improved from 0.04483 to 0.04473, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9862 - loss: 0.0505 - val_accuracy: 0.9854 - val_loss: 0.0447\n",
      "Epoch 387/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9860 - loss: 0.0577\n",
      "Epoch 387: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9850 - loss: 0.0531 - val_accuracy: 0.9846 - val_loss: 0.0484\n",
      "Epoch 388/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9920 - loss: 0.0423\n",
      "Epoch 388: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.0549 - val_accuracy: 0.9846 - val_loss: 0.0509\n",
      "Epoch 389/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0274\n",
      "Epoch 389: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9839 - loss: 0.0544 - val_accuracy: 0.9815 - val_loss: 0.0531\n",
      "Epoch 390/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9800 - loss: 0.1002\n",
      "Epoch 390: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9857 - loss: 0.0619 - val_accuracy: 0.9838 - val_loss: 0.0451\n",
      "Epoch 391/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0380\n",
      "Epoch 391: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9853 - loss: 0.0498 - val_accuracy: 0.9838 - val_loss: 0.0518\n",
      "Epoch 392/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9940 - loss: 0.0261\n",
      "Epoch 392: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9842 - loss: 0.0554 - val_accuracy: 0.9838 - val_loss: 0.0482\n",
      "Epoch 393/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9940 - loss: 0.0351\n",
      "Epoch 393: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0570 - val_accuracy: 0.9854 - val_loss: 0.0488\n",
      "Epoch 394/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9740 - loss: 0.0746\n",
      "Epoch 394: val_loss did not improve from 0.04473\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9792 - loss: 0.0665 - val_accuracy: 0.9838 - val_loss: 0.0526\n",
      "Epoch 395/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9740 - loss: 0.0706\n",
      "Epoch 395: val_loss improved from 0.04473 to 0.04429, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9806 - loss: 0.0651 - val_accuracy: 0.9846 - val_loss: 0.0443\n",
      "Epoch 396/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0387\n",
      "Epoch 396: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9865 - loss: 0.0535 - val_accuracy: 0.9838 - val_loss: 0.0482\n",
      "Epoch 397/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0432\n",
      "Epoch 397: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9859 - loss: 0.0547 - val_accuracy: 0.9838 - val_loss: 0.0466\n",
      "Epoch 398/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9820 - loss: 0.0677\n",
      "Epoch 398: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0594 - val_accuracy: 0.9823 - val_loss: 0.0498\n",
      "Epoch 399/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9920 - loss: 0.0316\n",
      "Epoch 399: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0443 - val_accuracy: 0.9854 - val_loss: 0.0452\n",
      "Epoch 400/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0467\n",
      "Epoch 400: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0556 - val_accuracy: 0.9846 - val_loss: 0.0474\n",
      "Epoch 401/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9860 - loss: 0.0384\n",
      "Epoch 401: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9858 - loss: 0.0522 - val_accuracy: 0.9854 - val_loss: 0.0462\n",
      "Epoch 402/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9860 - loss: 0.0517\n",
      "Epoch 402: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9856 - loss: 0.0511 - val_accuracy: 0.9862 - val_loss: 0.0484\n",
      "Epoch 403/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9780 - loss: 0.0723\n",
      "Epoch 403: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9821 - loss: 0.0608 - val_accuracy: 0.9838 - val_loss: 0.0514\n",
      "Epoch 404/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9720 - loss: 0.0678\n",
      "Epoch 404: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9820 - loss: 0.0580 - val_accuracy: 0.9846 - val_loss: 0.0476\n",
      "Epoch 405/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9820 - loss: 0.0448\n",
      "Epoch 405: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9858 - loss: 0.0510 - val_accuracy: 0.9862 - val_loss: 0.0479\n",
      "Epoch 406/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.0443\n",
      "Epoch 406: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9836 - loss: 0.0591 - val_accuracy: 0.9846 - val_loss: 0.0479\n",
      "Epoch 407/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9860 - loss: 0.0367\n",
      "Epoch 407: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0534 - val_accuracy: 0.9846 - val_loss: 0.0450\n",
      "Epoch 408/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9820 - loss: 0.0470\n",
      "Epoch 408: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9829 - loss: 0.0550 - val_accuracy: 0.9846 - val_loss: 0.0459\n",
      "Epoch 409/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9840 - loss: 0.0647\n",
      "Epoch 409: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0558 - val_accuracy: 0.9854 - val_loss: 0.0448\n",
      "Epoch 410/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9780 - loss: 0.0848\n",
      "Epoch 410: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9827 - loss: 0.0625 - val_accuracy: 0.9838 - val_loss: 0.0471\n",
      "Epoch 411/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9780 - loss: 0.0867\n",
      "Epoch 411: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9829 - loss: 0.0629 - val_accuracy: 0.9808 - val_loss: 0.0553\n",
      "Epoch 412/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9760 - loss: 0.0766\n",
      "Epoch 412: val_loss did not improve from 0.04429\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9828 - loss: 0.0607 - val_accuracy: 0.9846 - val_loss: 0.0471\n",
      "Epoch 413/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9900 - loss: 0.0580\n",
      "Epoch 413: val_loss improved from 0.04429 to 0.04408, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9870 - loss: 0.0511 - val_accuracy: 0.9846 - val_loss: 0.0441\n",
      "Epoch 414/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9920 - loss: 0.0273\n",
      "Epoch 414: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9893 - loss: 0.0417 - val_accuracy: 0.9838 - val_loss: 0.0506\n",
      "Epoch 415/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9820 - loss: 0.0662\n",
      "Epoch 415: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0600 - val_accuracy: 0.9846 - val_loss: 0.0456\n",
      "Epoch 416/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9900 - loss: 0.0395\n",
      "Epoch 416: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9852 - loss: 0.0523 - val_accuracy: 0.9846 - val_loss: 0.0459\n",
      "Epoch 417/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9940 - loss: 0.0348\n",
      "Epoch 417: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9868 - loss: 0.0541 - val_accuracy: 0.9854 - val_loss: 0.0448\n",
      "Epoch 418/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.0700\n",
      "Epoch 418: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9823 - loss: 0.0643 - val_accuracy: 0.9854 - val_loss: 0.0444\n",
      "Epoch 419/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9920 - loss: 0.0278\n",
      "Epoch 419: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9864 - loss: 0.0488 - val_accuracy: 0.9862 - val_loss: 0.0502\n",
      "Epoch 420/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0398\n",
      "Epoch 420: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0496 - val_accuracy: 0.9854 - val_loss: 0.0478\n",
      "Epoch 421/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9880 - loss: 0.0458\n",
      "Epoch 421: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9828 - loss: 0.0566 - val_accuracy: 0.9854 - val_loss: 0.0465\n",
      "Epoch 422/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9840 - loss: 0.0486\n",
      "Epoch 422: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9847 - loss: 0.0542 - val_accuracy: 0.9854 - val_loss: 0.0461\n",
      "Epoch 423/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9800 - loss: 0.0479\n",
      "Epoch 423: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9826 - loss: 0.0559 - val_accuracy: 0.9846 - val_loss: 0.0447\n",
      "Epoch 424/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9820 - loss: 0.0603\n",
      "Epoch 424: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9844 - loss: 0.0544 - val_accuracy: 0.9854 - val_loss: 0.0464\n",
      "Epoch 425/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0373\n",
      "Epoch 425: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9844 - loss: 0.0534 - val_accuracy: 0.9854 - val_loss: 0.0447\n",
      "Epoch 426/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9880 - loss: 0.0371\n",
      "Epoch 426: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9867 - loss: 0.0490 - val_accuracy: 0.9846 - val_loss: 0.0448\n",
      "Epoch 427/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9840 - loss: 0.0575\n",
      "Epoch 427: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9855 - loss: 0.0546 - val_accuracy: 0.9854 - val_loss: 0.0451\n",
      "Epoch 428/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9820 - loss: 0.0582\n",
      "Epoch 428: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9838 - loss: 0.0603 - val_accuracy: 0.9846 - val_loss: 0.0463\n",
      "Epoch 429/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9880 - loss: 0.0659\n",
      "Epoch 429: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9859 - loss: 0.0607 - val_accuracy: 0.9808 - val_loss: 0.0562\n",
      "Epoch 430/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9840 - loss: 0.0524\n",
      "Epoch 430: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9842 - loss: 0.0526 - val_accuracy: 0.9823 - val_loss: 0.0517\n",
      "Epoch 431/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9800 - loss: 0.0830\n",
      "Epoch 431: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9832 - loss: 0.0617 - val_accuracy: 0.9815 - val_loss: 0.0543\n",
      "Epoch 432/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9820 - loss: 0.0716\n",
      "Epoch 432: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.0622 - val_accuracy: 0.9800 - val_loss: 0.0553\n",
      "Epoch 433/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9880 - loss: 0.0339\n",
      "Epoch 433: val_loss did not improve from 0.04408\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0575 - val_accuracy: 0.9815 - val_loss: 0.0528\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(36, input_dim=12, activation='relu'))\n",
    "model3.add(Dense(12, activation='relu'))\n",
    "model3.add(Dense(8, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam', metrics=['accuracy'])\n",
    "history3 = model3.fit(X_train, y_train, epochs=2000,\n",
    "                      batch_size=500, validation_split=0.25, callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation_split  \n",
    "검증 데이터로 사용될 훈련 데이터의 비율입니다. 모델은 훈련 데이터의 이 부분을 구분하고,각 에포크가 끝날 때 이 데이터에 대한 손실과 모델 측정항목을 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x244dddd5b90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.7503207325935364,\n",
       "  0.8675904273986816,\n",
       "  0.8911983370780945,\n",
       "  0.9094175100326538,\n",
       "  0.9178855419158936,\n",
       "  0.925070583820343,\n",
       "  0.925070583820343,\n",
       "  0.9258403778076172,\n",
       "  0.9273800253868103,\n",
       "  0.9289196729660034,\n",
       "  0.9289196729660034,\n",
       "  0.9289196729660034,\n",
       "  0.9307159185409546,\n",
       "  0.9309725165367126,\n",
       "  0.9314857721328735,\n",
       "  0.9309725165367126,\n",
       "  0.9325121641159058,\n",
       "  0.9332820177078247,\n",
       "  0.9317423701286316,\n",
       "  0.9332820177078247,\n",
       "  0.9340518116950989,\n",
       "  0.9332820177078247,\n",
       "  0.9348216652870178,\n",
       "  0.9353348612785339,\n",
       "  0.9363613128662109,\n",
       "  0.937900960445404,\n",
       "  0.937644362449646,\n",
       "  0.9391840100288391,\n",
       "  0.9394406080245972,\n",
       "  0.9394406080245972,\n",
       "  0.9420066475868225,\n",
       "  0.9412368535995483,\n",
       "  0.9435462951660156,\n",
       "  0.9427765011787415,\n",
       "  0.9440595507621765,\n",
       "  0.9458557963371277,\n",
       "  0.9473954439163208,\n",
       "  0.9463689923286438,\n",
       "  0.9468821883201599,\n",
       "  0.9489350914955139,\n",
       "  0.9499614834785461,\n",
       "  0.949191689491272,\n",
       "  0.9509879350662231,\n",
       "  0.9497048854827881,\n",
       "  0.9515011310577393,\n",
       "  0.9540672302246094,\n",
       "  0.9543238282203674,\n",
       "  0.9527841806411743,\n",
       "  0.9556068778038025,\n",
       "  0.9540672302246094,\n",
       "  0.9581729769706726,\n",
       "  0.9558634757995605,\n",
       "  0.9576597213745117,\n",
       "  0.9597126245498657,\n",
       "  0.9604824185371399,\n",
       "  0.9604824185371399,\n",
       "  0.9586861729621887,\n",
       "  0.9594559669494629,\n",
       "  0.962022066116333,\n",
       "  0.9640749096870422,\n",
       "  0.962791919708252,\n",
       "  0.9648447632789612,\n",
       "  0.9674108028411865,\n",
       "  0.9622786641120911,\n",
       "  0.9638183116912842,\n",
       "  0.9648447632789612,\n",
       "  0.9676674604415894,\n",
       "  0.9686938524246216,\n",
       "  0.9692071080207825,\n",
       "  0.9697203040122986,\n",
       "  0.9679240584373474,\n",
       "  0.9710033535957336,\n",
       "  0.9679240584373474,\n",
       "  0.9692071080207825,\n",
       "  0.9694637060165405,\n",
       "  0.9720297455787659,\n",
       "  0.9715165495872498,\n",
       "  0.9707467555999756,\n",
       "  0.9715165495872498,\n",
       "  0.9710033535957336,\n",
       "  0.9715165495872498,\n",
       "  0.9720297455787659,\n",
       "  0.973825991153717,\n",
       "  0.974852442741394,\n",
       "  0.9730561971664429,\n",
       "  0.9730561971664429,\n",
       "  0.9743392467498779,\n",
       "  0.975622296333313,\n",
       "  0.973569393157959,\n",
       "  0.975878894329071,\n",
       "  0.9733127951622009,\n",
       "  0.9751090407371521,\n",
       "  0.9763920903205872,\n",
       "  0.9769052863121033,\n",
       "  0.9751090407371521,\n",
       "  0.9774185419082642,\n",
       "  0.9781883358955383,\n",
       "  0.9769052863121033,\n",
       "  0.9781883358955383,\n",
       "  0.9769052863121033,\n",
       "  0.973825991153717,\n",
       "  0.9761354923248291,\n",
       "  0.975622296333313,\n",
       "  0.9779317378997803,\n",
       "  0.9792147874832153,\n",
       "  0.9792147874832153,\n",
       "  0.9794713854789734,\n",
       "  0.9784449338912964,\n",
       "  0.9804978370666504,\n",
       "  0.9797279834747314,\n",
       "  0.9774185419082642,\n",
       "  0.9799845814704895,\n",
       "  0.9799845814704895,\n",
       "  0.9797279834747314,\n",
       "  0.9792147874832153,\n",
       "  0.9779317378997803,\n",
       "  0.9787015914916992,\n",
       "  0.9784449338912964,\n",
       "  0.9812676310539246,\n",
       "  0.9804978370666504,\n",
       "  0.9794713854789734,\n",
       "  0.9804978370666504,\n",
       "  0.9804978370666504,\n",
       "  0.9820374846458435,\n",
       "  0.9807544350624084,\n",
       "  0.9781883358955383,\n",
       "  0.9771619439125061,\n",
       "  0.9789581894874573,\n",
       "  0.9804978370666504,\n",
       "  0.9817808866500854,\n",
       "  0.9825506806373596,\n",
       "  0.9822940826416016,\n",
       "  0.9802412390708923,\n",
       "  0.9833204746246338,\n",
       "  0.9822940826416016,\n",
       "  0.9825506806373596,\n",
       "  0.9789581894874573,\n",
       "  0.9830638766288757,\n",
       "  0.9828072786331177,\n",
       "  0.9830638766288757,\n",
       "  0.9830638766288757,\n",
       "  0.9828072786331177,\n",
       "  0.9822940826416016,\n",
       "  0.9766486883163452,\n",
       "  0.9812676310539246,\n",
       "  0.9817808866500854,\n",
       "  0.9846035242080688,\n",
       "  0.9838337302207947,\n",
       "  0.9838337302207947,\n",
       "  0.9848601222038269,\n",
       "  0.9856299757957458,\n",
       "  0.9848601222038269,\n",
       "  0.9840903282165527,\n",
       "  0.9848601222038269,\n",
       "  0.9846035242080688,\n",
       "  0.9838337302207947,\n",
       "  0.9848601222038269,\n",
       "  0.9856299757957458,\n",
       "  0.986143171787262,\n",
       "  0.9848601222038269,\n",
       "  0.9848601222038269,\n",
       "  0.9846035242080688,\n",
       "  0.9866564273834229,\n",
       "  0.9848601222038269,\n",
       "  0.9848601222038269,\n",
       "  0.9851167798042297,\n",
       "  0.9858865737915039,\n",
       "  0.9851167798042297,\n",
       "  0.9856299757957458,\n",
       "  0.9853733777999878,\n",
       "  0.9851167798042297,\n",
       "  0.9846035242080688,\n",
       "  0.9866564273834229,\n",
       "  0.9858865737915039,\n",
       "  0.9866564273834229,\n",
       "  0.9840903282165527,\n",
       "  0.98639976978302,\n",
       "  0.986143171787262,\n",
       "  0.98639976978302,\n",
       "  0.9851167798042297,\n",
       "  0.986143171787262,\n",
       "  0.9856299757957458,\n",
       "  0.9856299757957458,\n",
       "  0.98639976978302,\n",
       "  0.98639976978302,\n",
       "  0.98639976978302,\n",
       "  0.9856299757957458,\n",
       "  0.9856299757957458,\n",
       "  0.987169623374939,\n",
       "  0.987169623374939,\n",
       "  0.9848601222038269,\n",
       "  0.98639976978302,\n",
       "  0.987169623374939,\n",
       "  0.9853733777999878,\n",
       "  0.9866564273834229,\n",
       "  0.9869130253791809,\n",
       "  0.986143171787262,\n",
       "  0.988196074962616,\n",
       "  0.9851167798042297,\n",
       "  0.98639976978302,\n",
       "  0.9851167798042297,\n",
       "  0.9856299757957458,\n",
       "  0.98639976978302,\n",
       "  0.9869130253791809,\n",
       "  0.9866564273834229,\n",
       "  0.98639976978302,\n",
       "  0.9869130253791809,\n",
       "  0.9876828193664551,\n",
       "  0.9869130253791809,\n",
       "  0.9866564273834229,\n",
       "  0.9879394173622131,\n",
       "  0.987169623374939,\n",
       "  0.987169623374939,\n",
       "  0.988196074962616,\n",
       "  0.987169623374939,\n",
       "  0.988452672958374,\n",
       "  0.9876828193664551,\n",
       "  0.987169623374939,\n",
       "  0.9866564273834229,\n",
       "  0.9879394173622131,\n",
       "  0.9876828193664551,\n",
       "  0.988196074962616,\n",
       "  0.987426221370697,\n",
       "  0.988452672958374,\n",
       "  0.988196074962616,\n",
       "  0.9876828193664551,\n",
       "  0.987426221370697,\n",
       "  0.987169623374939,\n",
       "  0.9869130253791809,\n",
       "  0.9887092709541321,\n",
       "  0.9876828193664551,\n",
       "  0.9887092709541321,\n",
       "  0.9897357225418091,\n",
       "  0.9879394173622131,\n",
       "  0.988452672958374,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.9892224669456482,\n",
       "  0.988452672958374,\n",
       "  0.987169623374939,\n",
       "  0.9879394173622131,\n",
       "  0.9876828193664551,\n",
       "  0.988452672958374,\n",
       "  0.9889658689498901,\n",
       "  0.988196074962616,\n",
       "  0.9887092709541321,\n",
       "  0.9876828193664551,\n",
       "  0.9897357225418091,\n",
       "  0.9894790649414062,\n",
       "  0.9899923205375671,\n",
       "  0.9887092709541321,\n",
       "  0.9899923205375671,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.9889658689498901,\n",
       "  0.9879394173622131,\n",
       "  0.9905055165290833,\n",
       "  0.9879394173622131,\n",
       "  0.988452672958374,\n",
       "  0.987426221370697,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.9876828193664551,\n",
       "  0.988452672958374,\n",
       "  0.988196074962616,\n",
       "  0.9869130253791809,\n",
       "  0.9887092709541321,\n",
       "  0.9905055165290833,\n",
       "  0.988452672958374,\n",
       "  0.9887092709541321,\n",
       "  0.9907621145248413,\n",
       "  0.986143171787262,\n",
       "  0.988196074962616,\n",
       "  0.9894790649414062,\n",
       "  0.9899923205375671,\n",
       "  0.9897357225418091,\n",
       "  0.9894790649414062,\n",
       "  0.9869130253791809,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.9899923205375671,\n",
       "  0.9899923205375671,\n",
       "  0.9897357225418091,\n",
       "  0.988196074962616,\n",
       "  0.987169623374939,\n",
       "  0.9876828193664551,\n",
       "  0.9905055165290833,\n",
       "  0.9907621145248413,\n",
       "  0.9892224669456482,\n",
       "  0.9905055165290833,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9894790649414062,\n",
       "  0.9897357225418091,\n",
       "  0.9889658689498901,\n",
       "  0.9897357225418091,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.9892224669456482,\n",
       "  0.988196074962616,\n",
       "  0.9887092709541321,\n",
       "  0.988196074962616,\n",
       "  0.9905055165290833,\n",
       "  0.9907621145248413,\n",
       "  0.9897357225418091,\n",
       "  0.9907621145248413,\n",
       "  0.9876828193664551,\n",
       "  0.9897357225418091,\n",
       "  0.9905055165290833,\n",
       "  0.9897357225418091,\n",
       "  0.9905055165290833,\n",
       "  0.988196074962616,\n",
       "  0.9889658689498901,\n",
       "  0.9894790649414062,\n",
       "  0.9902489185333252,\n",
       "  0.9905055165290833,\n",
       "  0.9899923205375671,\n",
       "  0.9905055165290833,\n",
       "  0.9902489185333252,\n",
       "  0.9905055165290833,\n",
       "  0.9905055165290833,\n",
       "  0.98639976978302,\n",
       "  0.9894790649414062,\n",
       "  0.9910187125205994,\n",
       "  0.9912753105163574,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9892224669456482,\n",
       "  0.9879394173622131,\n",
       "  0.9879394173622131,\n",
       "  0.9887092709541321,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.987426221370697,\n",
       "  0.9899923205375671,\n",
       "  0.9897357225418091,\n",
       "  0.9912753105163574,\n",
       "  0.9899923205375671,\n",
       "  0.9907621145248413,\n",
       "  0.9905055165290833,\n",
       "  0.9915319681167603,\n",
       "  0.9899923205375671,\n",
       "  0.9912753105163574,\n",
       "  0.9905055165290833,\n",
       "  0.9910187125205994,\n",
       "  0.9905055165290833,\n",
       "  0.9915319681167603,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.988452672958374,\n",
       "  0.9830638766288757,\n",
       "  0.9830638766288757,\n",
       "  0.987426221370697,\n",
       "  0.9869130253791809,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.9915319681167603,\n",
       "  0.9902489185333252,\n",
       "  0.9910187125205994,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9907621145248413,\n",
       "  0.9892224669456482,\n",
       "  0.9902489185333252,\n",
       "  0.9907621145248413,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9910187125205994,\n",
       "  0.9905055165290833,\n",
       "  0.9910187125205994,\n",
       "  0.9907621145248413,\n",
       "  0.9925583600997925,\n",
       "  0.9902489185333252,\n",
       "  0.9915319681167603,\n",
       "  0.9923017621040344,\n",
       "  0.9915319681167603,\n",
       "  0.9923017621040344,\n",
       "  0.9897357225418091,\n",
       "  0.9879394173622131,\n",
       "  0.9887092709541321,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.9920451641082764,\n",
       "  0.9915319681167603,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9910187125205994,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9912753105163574,\n",
       "  0.9917885661125183,\n",
       "  0.9899923205375671,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9902489185333252,\n",
       "  0.9897357225418091,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.9897357225418091,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9897357225418091,\n",
       "  0.9917885661125183,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9917885661125183,\n",
       "  0.9905055165290833,\n",
       "  0.9876828193664551,\n",
       "  0.9840903282165527,\n",
       "  0.9897357225418091,\n",
       "  0.9902489185333252,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.988452672958374,\n",
       "  0.9907621145248413,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9928149580955505,\n",
       "  0.9902489185333252,\n",
       "  0.9915319681167603,\n",
       "  0.9905055165290833,\n",
       "  0.9897357225418091,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9897357225418091,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9928149580955505,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9899923205375671,\n",
       "  0.9902489185333252,\n",
       "  0.9917885661125183,\n",
       "  0.9912753105163574,\n",
       "  0.9925583600997925,\n",
       "  0.9920451641082764,\n",
       "  0.9910187125205994,\n",
       "  0.9912753105163574,\n",
       "  0.9899923205375671,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9905055165290833,\n",
       "  0.9928149580955505,\n",
       "  0.9910187125205994,\n",
       "  0.9905055165290833,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9905055165290833,\n",
       "  0.9923017621040344,\n",
       "  0.9889658689498901,\n",
       "  0.9889658689498901,\n",
       "  0.9930716156959534,\n",
       "  0.9920451641082764,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9887092709541321,\n",
       "  0.9902489185333252,\n",
       "  0.9905055165290833,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.9928149580955505,\n",
       "  0.9930716156959534,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9899923205375671,\n",
       "  0.9917885661125183,\n",
       "  0.9905055165290833,\n",
       "  0.9923017621040344,\n",
       "  0.9902489185333252,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9920451641082764,\n",
       "  0.9905055165290833,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9899923205375671,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9910187125205994,\n",
       "  0.9892224669456482,\n",
       "  0.9897357225418091,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9935848116874695,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9925583600997925,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9912753105163574,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9930716156959534,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9912753105163574,\n",
       "  0.9925583600997925,\n",
       "  0.9925583600997925,\n",
       "  0.9905055165290833,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9897357225418091,\n",
       "  0.9907621145248413,\n",
       "  0.9894790649414062,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9920451641082764,\n",
       "  0.9930716156959534,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9920451641082764,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9933282136917114,\n",
       "  0.9928149580955505,\n",
       "  0.9925583600997925,\n",
       "  0.9905055165290833,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.988196074962616,\n",
       "  0.9899923205375671,\n",
       "  0.9894790649414062,\n",
       "  0.9917885661125183,\n",
       "  0.9928149580955505,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9897357225418091,\n",
       "  0.9925583600997925,\n",
       "  0.9925583600997925,\n",
       "  0.9930716156959534,\n",
       "  0.9920451641082764,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9915319681167603,\n",
       "  0.9912753105163574,\n",
       "  0.9933282136917114,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9892224669456482,\n",
       "  0.9899923205375671,\n",
       "  0.9897357225418091,\n",
       "  0.9889658689498901,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9910187125205994,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9920451641082764,\n",
       "  0.9933282136917114,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9933282136917114,\n",
       "  0.9930716156959534,\n",
       "  0.9928149580955505,\n",
       "  0.9912753105163574,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9915319681167603,\n",
       "  0.9899923205375671,\n",
       "  0.9887092709541321,\n",
       "  0.988196074962616,\n",
       "  0.9897357225418091,\n",
       "  0.9912753105163574,\n",
       "  0.9917885661125183,\n",
       "  0.9933282136917114,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9894790649414062,\n",
       "  0.9899923205375671,\n",
       "  0.9915319681167603,\n",
       "  0.9889658689498901,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9899923205375671,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9897357225418091,\n",
       "  0.9902489185333252,\n",
       "  0.9910187125205994,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9899923205375671,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9925583600997925,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9933282136917114,\n",
       "  0.9933282136917114,\n",
       "  0.9928149580955505,\n",
       "  0.9940980076789856,\n",
       "  0.9923017621040344,\n",
       "  0.9925583600997925,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9933282136917114,\n",
       "  0.9930716156959534,\n",
       "  0.9935848116874695,\n",
       "  0.9920451641082764,\n",
       "  0.9899923205375671,\n",
       "  0.9907621145248413,\n",
       "  0.9902489185333252,\n",
       "  0.9899923205375671,\n",
       "  0.988196074962616,\n",
       "  0.9902489185333252,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9879394173622131,\n",
       "  0.9879394173622131,\n",
       "  0.9876828193664551,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9930716156959534,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9930716156959534,\n",
       "  0.9917885661125183,\n",
       "  0.9935848116874695,\n",
       "  0.9933282136917114,\n",
       "  0.9910187125205994,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9938414096832275,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9935848116874695,\n",
       "  0.9938414096832275,\n",
       "  0.9935848116874695,\n",
       "  0.9930716156959534,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9930716156959534,\n",
       "  0.9938414096832275,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9930716156959534,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9915319681167603,\n",
       "  0.9928149580955505,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9930716156959534,\n",
       "  0.9915319681167603,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9925583600997925,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9930716156959534,\n",
       "  0.9923017621040344,\n",
       "  0.9894790649414062,\n",
       "  0.9879394173622131,\n",
       "  0.9917885661125183,\n",
       "  0.9928149580955505,\n",
       "  0.9935848116874695,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9935848116874695,\n",
       "  0.9938414096832275,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9933282136917114,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9928149580955505,\n",
       "  0.9910187125205994,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9940980076789856,\n",
       "  0.9910187125205994,\n",
       "  0.9899923205375671,\n",
       "  0.9930716156959534,\n",
       "  0.9940980076789856,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9933282136917114,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9938414096832275,\n",
       "  0.9935848116874695,\n",
       "  0.9930716156959534,\n",
       "  0.9920451641082764,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9930716156959534,\n",
       "  0.9933282136917114,\n",
       "  0.9938414096832275,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9935848116874695,\n",
       "  0.9915319681167603,\n",
       "  0.9902489185333252,\n",
       "  0.9917885661125183,\n",
       "  0.9933282136917114,\n",
       "  0.9930716156959534,\n",
       "  0.9933282136917114,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9894790649414062,\n",
       "  0.9933282136917114,\n",
       "  0.9930716156959534,\n",
       "  0.9935848116874695,\n",
       "  0.9925583600997925,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9920451641082764,\n",
       "  0.987426221370697,\n",
       "  0.9912753105163574,\n",
       "  0.9935848116874695,\n",
       "  0.9923017621040344,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9930716156959534,\n",
       "  0.9923017621040344,\n",
       "  0.9905055165290833,\n",
       "  0.9928149580955505,\n",
       "  0.9933282136917114,\n",
       "  0.9938414096832275,\n",
       "  0.9930716156959534,\n",
       "  0.9920451641082764,\n",
       "  0.9933282136917114,\n",
       "  0.9902489185333252,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9894790649414062,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9938414096832275,\n",
       "  0.9915319681167603,\n",
       "  0.9920451641082764,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9930716156959534,\n",
       "  0.9905055165290833,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9943546056747437,\n",
       "  0.9933282136917114,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9917885661125183,\n",
       "  0.9938414096832275,\n",
       "  0.9907621145248413,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9925583600997925,\n",
       "  0.9930716156959534,\n",
       "  0.9912753105163574,\n",
       "  0.9935848116874695,\n",
       "  0.9933282136917114,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9894790649414062,\n",
       "  0.9899923205375671,\n",
       "  0.9920451641082764,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9923017621040344,\n",
       "  0.9933282136917114,\n",
       "  0.9912753105163574,\n",
       "  0.9930716156959534,\n",
       "  0.9907621145248413,\n",
       "  0.9925583600997925,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9912753105163574,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9943546056747437,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9902489185333252,\n",
       "  0.9938414096832275,\n",
       "  0.9912753105163574,\n",
       "  0.9917885661125183,\n",
       "  0.9935848116874695,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9928149580955505,\n",
       "  0.9933282136917114,\n",
       "  0.9933282136917114,\n",
       "  0.9935848116874695,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9935848116874695,\n",
       "  0.9920451641082764,\n",
       "  0.9910187125205994,\n",
       "  0.9897357225418091,\n",
       "  0.9912753105163574,\n",
       "  0.9928149580955505,\n",
       "  0.9925583600997925,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9938414096832275,\n",
       "  0.9940980076789856,\n",
       "  0.9938414096832275,\n",
       "  0.9912753105163574,\n",
       "  0.9899923205375671,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9938414096832275,\n",
       "  0.9928149580955505,\n",
       "  0.9920451641082764,\n",
       "  0.9925583600997925,\n",
       "  0.9935848116874695,\n",
       "  0.9910187125205994,\n",
       "  0.9902489185333252,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9928149580955505,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9923017621040344,\n",
       "  0.9935848116874695,\n",
       "  0.9928149580955505,\n",
       "  0.9935848116874695,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9935848116874695,\n",
       "  0.9938414096832275,\n",
       "  0.9892224669456482,\n",
       "  0.9887092709541321,\n",
       "  0.9892224669456482,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9925583600997925,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9940980076789856,\n",
       "  0.9928149580955505,\n",
       "  ...],\n",
       " 'loss': [0.5320450663566589,\n",
       "  0.33483561873435974,\n",
       "  0.3182571232318878,\n",
       "  0.29344624280929565,\n",
       "  0.27022844552993774,\n",
       "  0.25283917784690857,\n",
       "  0.24317917227745056,\n",
       "  0.23677967488765717,\n",
       "  0.23087279498577118,\n",
       "  0.22575704753398895,\n",
       "  0.22160704433918,\n",
       "  0.21789444983005524,\n",
       "  0.21456192433834076,\n",
       "  0.21080976724624634,\n",
       "  0.20829598605632782,\n",
       "  0.20616818964481354,\n",
       "  0.20320503413677216,\n",
       "  0.19955451786518097,\n",
       "  0.19566257297992706,\n",
       "  0.19281502068042755,\n",
       "  0.19012893736362457,\n",
       "  0.18774782121181488,\n",
       "  0.1855396032333374,\n",
       "  0.182862788438797,\n",
       "  0.18041640520095825,\n",
       "  0.17767825722694397,\n",
       "  0.17422917485237122,\n",
       "  0.17047449946403503,\n",
       "  0.1678650677204132,\n",
       "  0.16455146670341492,\n",
       "  0.16108419001102448,\n",
       "  0.15776629745960236,\n",
       "  0.15428832173347473,\n",
       "  0.15021006762981415,\n",
       "  0.15108604729175568,\n",
       "  0.14617618918418884,\n",
       "  0.14140157401561737,\n",
       "  0.13889430463314056,\n",
       "  0.13778118789196014,\n",
       "  0.1363777220249176,\n",
       "  0.13233189284801483,\n",
       "  0.13068509101867676,\n",
       "  0.13009464740753174,\n",
       "  0.12768292427062988,\n",
       "  0.12698720395565033,\n",
       "  0.12433038651943207,\n",
       "  0.12141391634941101,\n",
       "  0.12433118373155594,\n",
       "  0.12263784557580948,\n",
       "  0.11758926510810852,\n",
       "  0.11605855822563171,\n",
       "  0.11410152167081833,\n",
       "  0.11580776423215866,\n",
       "  0.11771352589130402,\n",
       "  0.1142951175570488,\n",
       "  0.11388479918241501,\n",
       "  0.1156512200832367,\n",
       "  0.11105656623840332,\n",
       "  0.10796856880187988,\n",
       "  0.10515550523996353,\n",
       "  0.10633404552936554,\n",
       "  0.10613707453012466,\n",
       "  0.10271268337965012,\n",
       "  0.10374117642641068,\n",
       "  0.10101348161697388,\n",
       "  0.10246410220861435,\n",
       "  0.0995878130197525,\n",
       "  0.09770403802394867,\n",
       "  0.09689221531152725,\n",
       "  0.09639228880405426,\n",
       "  0.09538799524307251,\n",
       "  0.09419376403093338,\n",
       "  0.09317995607852936,\n",
       "  0.09241150319576263,\n",
       "  0.09217582643032074,\n",
       "  0.09144081175327301,\n",
       "  0.09051989018917084,\n",
       "  0.08971699327230453,\n",
       "  0.09037359058856964,\n",
       "  0.08885981887578964,\n",
       "  0.08898969739675522,\n",
       "  0.08737115561962128,\n",
       "  0.08549975603818893,\n",
       "  0.08517944067716599,\n",
       "  0.08457240462303162,\n",
       "  0.08391682803630829,\n",
       "  0.08327082544565201,\n",
       "  0.08216802775859833,\n",
       "  0.08146072924137115,\n",
       "  0.08163744956254959,\n",
       "  0.08209504932165146,\n",
       "  0.07946065068244934,\n",
       "  0.0787588432431221,\n",
       "  0.07796335965394974,\n",
       "  0.07766016572713852,\n",
       "  0.07657599449157715,\n",
       "  0.07604790478944778,\n",
       "  0.07815016061067581,\n",
       "  0.07749870419502258,\n",
       "  0.07618533074855804,\n",
       "  0.07954251766204834,\n",
       "  0.07700580358505249,\n",
       "  0.07506025582551956,\n",
       "  0.07258591800928116,\n",
       "  0.07204011082649231,\n",
       "  0.07161803543567657,\n",
       "  0.07159755378961563,\n",
       "  0.0705656036734581,\n",
       "  0.07013098150491714,\n",
       "  0.06915497034788132,\n",
       "  0.06990384310483932,\n",
       "  0.06833021342754364,\n",
       "  0.06732755154371262,\n",
       "  0.06731166690587997,\n",
       "  0.06788972020149231,\n",
       "  0.07249529659748077,\n",
       "  0.07068084180355072,\n",
       "  0.06856867671012878,\n",
       "  0.06456257402896881,\n",
       "  0.06367918848991394,\n",
       "  0.06400147825479507,\n",
       "  0.06430597603321075,\n",
       "  0.06350774317979813,\n",
       "  0.0619528591632843,\n",
       "  0.06489921361207962,\n",
       "  0.06596816331148148,\n",
       "  0.07180775701999664,\n",
       "  0.06721662729978561,\n",
       "  0.06356862932443619,\n",
       "  0.06165082007646561,\n",
       "  0.061669424176216125,\n",
       "  0.060213297605514526,\n",
       "  0.06143992394208908,\n",
       "  0.059448882937431335,\n",
       "  0.0589606836438179,\n",
       "  0.059727489948272705,\n",
       "  0.0654330849647522,\n",
       "  0.05935817211866379,\n",
       "  0.059120532125234604,\n",
       "  0.058078519999980927,\n",
       "  0.057407427579164505,\n",
       "  0.057721953839063644,\n",
       "  0.05944838374853134,\n",
       "  0.06515563279390335,\n",
       "  0.06118267774581909,\n",
       "  0.05967476963996887,\n",
       "  0.05579720437526703,\n",
       "  0.055160488933324814,\n",
       "  0.054516419768333435,\n",
       "  0.054378289729356766,\n",
       "  0.05371788516640663,\n",
       "  0.0542779304087162,\n",
       "  0.05371493101119995,\n",
       "  0.05342748388648033,\n",
       "  0.05419197678565979,\n",
       "  0.05323011055588722,\n",
       "  0.05246414244174957,\n",
       "  0.051803912967443466,\n",
       "  0.05269559845328331,\n",
       "  0.052598271518945694,\n",
       "  0.05285555496811867,\n",
       "  0.05373450741171837,\n",
       "  0.05263635143637657,\n",
       "  0.053193580359220505,\n",
       "  0.05198400840163231,\n",
       "  0.05321211740374565,\n",
       "  0.05131103843450546,\n",
       "  0.049833234399557114,\n",
       "  0.049801114946603775,\n",
       "  0.05047501623630524,\n",
       "  0.0506957583129406,\n",
       "  0.05103617161512375,\n",
       "  0.049077730625867844,\n",
       "  0.049235664308071136,\n",
       "  0.04885600879788399,\n",
       "  0.05076700448989868,\n",
       "  0.04802042618393898,\n",
       "  0.04757904261350632,\n",
       "  0.049027811735868454,\n",
       "  0.047364842146635056,\n",
       "  0.048371151089668274,\n",
       "  0.047039780765771866,\n",
       "  0.04711448401212692,\n",
       "  0.04829532280564308,\n",
       "  0.04988797381520271,\n",
       "  0.0491613894701004,\n",
       "  0.04861782491207123,\n",
       "  0.046458132565021515,\n",
       "  0.045842286199331284,\n",
       "  0.04544557258486748,\n",
       "  0.045646440237760544,\n",
       "  0.050345923751592636,\n",
       "  0.04709802195429802,\n",
       "  0.04655566066503525,\n",
       "  0.0460943840444088,\n",
       "  0.04719184711575508,\n",
       "  0.04919708892703056,\n",
       "  0.04452904686331749,\n",
       "  0.04766984283924103,\n",
       "  0.045349594205617905,\n",
       "  0.047103021293878555,\n",
       "  0.04656315967440605,\n",
       "  0.04362474009394646,\n",
       "  0.04405977204442024,\n",
       "  0.04394291341304779,\n",
       "  0.04465671256184578,\n",
       "  0.04302088916301727,\n",
       "  0.04245663061738014,\n",
       "  0.0445857010781765,\n",
       "  0.044202741235494614,\n",
       "  0.04179487004876137,\n",
       "  0.043073274195194244,\n",
       "  0.04179712384939194,\n",
       "  0.04116562008857727,\n",
       "  0.04346232861280441,\n",
       "  0.040534328669309616,\n",
       "  0.04077167809009552,\n",
       "  0.043188296258449554,\n",
       "  0.044869568198919296,\n",
       "  0.0411062128841877,\n",
       "  0.04086767137050629,\n",
       "  0.04054415225982666,\n",
       "  0.04229297116398811,\n",
       "  0.04139069840312004,\n",
       "  0.04087785631418228,\n",
       "  0.040225882083177567,\n",
       "  0.04050483554601669,\n",
       "  0.04171348735690117,\n",
       "  0.043546102941036224,\n",
       "  0.04129697382450104,\n",
       "  0.039487097412347794,\n",
       "  0.039251744747161865,\n",
       "  0.03870789334177971,\n",
       "  0.04007277265191078,\n",
       "  0.040456339716911316,\n",
       "  0.03823501244187355,\n",
       "  0.03817731514573097,\n",
       "  0.03794679790735245,\n",
       "  0.03781824931502342,\n",
       "  0.041675418615341187,\n",
       "  0.040756456553936005,\n",
       "  0.040200669318437576,\n",
       "  0.039878904819488525,\n",
       "  0.03995369374752045,\n",
       "  0.039298444986343384,\n",
       "  0.03952743858098984,\n",
       "  0.03884157910943031,\n",
       "  0.03776302933692932,\n",
       "  0.037923961877822876,\n",
       "  0.039134763181209564,\n",
       "  0.0392257384955883,\n",
       "  0.03718588501214981,\n",
       "  0.04075444117188454,\n",
       "  0.03969814255833626,\n",
       "  0.0395059697329998,\n",
       "  0.041067518293857574,\n",
       "  0.03776712715625763,\n",
       "  0.038839761167764664,\n",
       "  0.03953787311911583,\n",
       "  0.0410284660756588,\n",
       "  0.03716249763965607,\n",
       "  0.03674158453941345,\n",
       "  0.037767194211483,\n",
       "  0.03978576138615608,\n",
       "  0.03886118903756142,\n",
       "  0.04238515347242355,\n",
       "  0.03903413563966751,\n",
       "  0.037309300154447556,\n",
       "  0.03739769756793976,\n",
       "  0.03604782000184059,\n",
       "  0.036189381033182144,\n",
       "  0.043897595256567,\n",
       "  0.041410621255636215,\n",
       "  0.03629959747195244,\n",
       "  0.03629434481263161,\n",
       "  0.037481166422367096,\n",
       "  0.038102488964796066,\n",
       "  0.03984331339597702,\n",
       "  0.03880736604332924,\n",
       "  0.03606487438082695,\n",
       "  0.03492503613233566,\n",
       "  0.03496025875210762,\n",
       "  0.037849292159080505,\n",
       "  0.03648225590586662,\n",
       "  0.040930021554231644,\n",
       "  0.04229109734296799,\n",
       "  0.036797765642404556,\n",
       "  0.03667043149471283,\n",
       "  0.03540747985243797,\n",
       "  0.034625113010406494,\n",
       "  0.0343199223279953,\n",
       "  0.03435467556118965,\n",
       "  0.036419738084077835,\n",
       "  0.035048868507146835,\n",
       "  0.036628905683755875,\n",
       "  0.03506013751029968,\n",
       "  0.03501998260617256,\n",
       "  0.03538675606250763,\n",
       "  0.034388914704322815,\n",
       "  0.03488908335566521,\n",
       "  0.03491344675421715,\n",
       "  0.03524652495980263,\n",
       "  0.034418556839227676,\n",
       "  0.03851044178009033,\n",
       "  0.03964726999402046,\n",
       "  0.036799900233745575,\n",
       "  0.034098438918590546,\n",
       "  0.034060440957546234,\n",
       "  0.03644376993179321,\n",
       "  0.03333807736635208,\n",
       "  0.0367484875023365,\n",
       "  0.03643769398331642,\n",
       "  0.03439803794026375,\n",
       "  0.03652854636311531,\n",
       "  0.035053420811891556,\n",
       "  0.04059780016541481,\n",
       "  0.03873990476131439,\n",
       "  0.03866518661379814,\n",
       "  0.03450940176844597,\n",
       "  0.03275790065526962,\n",
       "  0.034662455320358276,\n",
       "  0.034279827028512955,\n",
       "  0.03416535630822182,\n",
       "  0.03364747017621994,\n",
       "  0.0355127714574337,\n",
       "  0.040412675589323044,\n",
       "  0.03595962002873421,\n",
       "  0.03556465357542038,\n",
       "  0.03376476839184761,\n",
       "  0.03329271823167801,\n",
       "  0.03446264937520027,\n",
       "  0.03537806496024132,\n",
       "  0.03896850347518921,\n",
       "  0.04029109328985214,\n",
       "  0.03853590413928032,\n",
       "  0.03291168436408043,\n",
       "  0.03324306011199951,\n",
       "  0.03494475036859512,\n",
       "  0.03926082327961922,\n",
       "  0.035533446818590164,\n",
       "  0.03274266794323921,\n",
       "  0.032571662217378616,\n",
       "  0.032648831605911255,\n",
       "  0.031818687915802,\n",
       "  0.03278358653187752,\n",
       "  0.032672032713890076,\n",
       "  0.03423202410340309,\n",
       "  0.03259503096342087,\n",
       "  0.03179396316409111,\n",
       "  0.03377465903759003,\n",
       "  0.03329843282699585,\n",
       "  0.03209060803055763,\n",
       "  0.033125732094049454,\n",
       "  0.03462130203843117,\n",
       "  0.03831160441040993,\n",
       "  0.04650405794382095,\n",
       "  0.0544976107776165,\n",
       "  0.04116281494498253,\n",
       "  0.04111923277378082,\n",
       "  0.03303258866071701,\n",
       "  0.03398875519633293,\n",
       "  0.03237198293209076,\n",
       "  0.03293953090906143,\n",
       "  0.03291267156600952,\n",
       "  0.033769480884075165,\n",
       "  0.03355860710144043,\n",
       "  0.033083997666835785,\n",
       "  0.031990792602300644,\n",
       "  0.03277317062020302,\n",
       "  0.03418763354420662,\n",
       "  0.03256670758128166,\n",
       "  0.03121296316385269,\n",
       "  0.0313497893512249,\n",
       "  0.03160998970270157,\n",
       "  0.03287063166499138,\n",
       "  0.034231748431921005,\n",
       "  0.03164895996451378,\n",
       "  0.031337276101112366,\n",
       "  0.031057287007570267,\n",
       "  0.032583825290203094,\n",
       "  0.032029956579208374,\n",
       "  0.031890619546175,\n",
       "  0.03182371333241463,\n",
       "  0.031913019716739655,\n",
       "  0.03528030961751938,\n",
       "  0.03977716714143753,\n",
       "  0.037794146686792374,\n",
       "  0.03156070411205292,\n",
       "  0.03237941116094589,\n",
       "  0.03143276646733284,\n",
       "  0.03163515776395798,\n",
       "  0.030975861474871635,\n",
       "  0.03182365372776985,\n",
       "  0.03164328634738922,\n",
       "  0.03266815096139908,\n",
       "  0.030912170186638832,\n",
       "  0.031070033088326454,\n",
       "  0.031009215861558914,\n",
       "  0.03034655749797821,\n",
       "  0.03061026707291603,\n",
       "  0.03142644464969635,\n",
       "  0.03221817687153816,\n",
       "  0.03302450850605965,\n",
       "  0.03116575814783573,\n",
       "  0.031230930238962173,\n",
       "  0.03087911568582058,\n",
       "  0.030193191021680832,\n",
       "  0.03102095052599907,\n",
       "  0.03294211998581886,\n",
       "  0.034606557339429855,\n",
       "  0.03152267634868622,\n",
       "  0.03042924404144287,\n",
       "  0.030059102922677994,\n",
       "  0.03163391724228859,\n",
       "  0.029866809025406837,\n",
       "  0.032858774065971375,\n",
       "  0.032556258141994476,\n",
       "  0.03122316487133503,\n",
       "  0.02992517314851284,\n",
       "  0.03298991918563843,\n",
       "  0.033536646515131,\n",
       "  0.03444706276059151,\n",
       "  0.03324240446090698,\n",
       "  0.031338129192590714,\n",
       "  0.0305006206035614,\n",
       "  0.03381920978426933,\n",
       "  0.033846426755189896,\n",
       "  0.03264923021197319,\n",
       "  0.032490264624357224,\n",
       "  0.030583135783672333,\n",
       "  0.03257649391889572,\n",
       "  0.03937569633126259,\n",
       "  0.048765987157821655,\n",
       "  0.037067711353302,\n",
       "  0.03294135630130768,\n",
       "  0.029968833550810814,\n",
       "  0.030521003529429436,\n",
       "  0.030416235327720642,\n",
       "  0.03487132489681244,\n",
       "  0.03290824219584465,\n",
       "  0.03036913275718689,\n",
       "  0.030384572222828865,\n",
       "  0.030582649633288383,\n",
       "  0.030467798933386803,\n",
       "  0.029944896697998047,\n",
       "  0.032249774783849716,\n",
       "  0.030453408136963844,\n",
       "  0.030055293813347816,\n",
       "  0.03178417682647705,\n",
       "  0.03135822340846062,\n",
       "  0.03127532824873924,\n",
       "  0.030377861112356186,\n",
       "  0.030152879655361176,\n",
       "  0.029618779197335243,\n",
       "  0.02962169609963894,\n",
       "  0.030734790489077568,\n",
       "  0.03188883140683174,\n",
       "  0.03064282052218914,\n",
       "  0.030753036960959435,\n",
       "  0.032826364040374756,\n",
       "  0.029935011640191078,\n",
       "  0.03165379911661148,\n",
       "  0.030064765363931656,\n",
       "  0.030980369076132774,\n",
       "  0.029736431315541267,\n",
       "  0.03226522356271744,\n",
       "  0.032490137964487076,\n",
       "  0.029730066657066345,\n",
       "  0.030490735545754433,\n",
       "  0.02921917475759983,\n",
       "  0.031372055411338806,\n",
       "  0.0347764678299427,\n",
       "  0.029837526381015778,\n",
       "  0.029767708852887154,\n",
       "  0.029839185997843742,\n",
       "  0.030690018087625504,\n",
       "  0.030267102643847466,\n",
       "  0.029204443097114563,\n",
       "  0.029948290437459946,\n",
       "  0.03179845213890076,\n",
       "  0.03148641437292099,\n",
       "  0.03265075385570526,\n",
       "  0.0331086702644825,\n",
       "  0.029781298711895943,\n",
       "  0.03056095726788044,\n",
       "  0.03006424754858017,\n",
       "  0.03013995662331581,\n",
       "  0.031045200303196907,\n",
       "  0.03161909803748131,\n",
       "  0.03404659777879715,\n",
       "  0.030552009120583534,\n",
       "  0.029863422736525536,\n",
       "  0.02901529148221016,\n",
       "  0.029882898554205894,\n",
       "  0.029961124062538147,\n",
       "  0.03016427345573902,\n",
       "  0.03052481822669506,\n",
       "  0.03376847505569458,\n",
       "  0.0326915942132473,\n",
       "  0.029313476756215096,\n",
       "  0.031162209808826447,\n",
       "  0.03438902646303177,\n",
       "  0.029618000611662865,\n",
       "  0.029074417427182198,\n",
       "  0.03020019829273224,\n",
       "  0.031066512688994408,\n",
       "  0.029864275828003883,\n",
       "  0.0367448590695858,\n",
       "  0.03493521735072136,\n",
       "  0.028417745605111122,\n",
       "  0.028826666995882988,\n",
       "  0.028551660478115082,\n",
       "  0.029183197766542435,\n",
       "  0.03205719217658043,\n",
       "  0.030328072607517242,\n",
       "  0.032412394881248474,\n",
       "  0.03434444218873978,\n",
       "  0.03130533546209335,\n",
       "  0.032571177929639816,\n",
       "  0.030292877927422523,\n",
       "  0.02915249392390251,\n",
       "  0.02833443693816662,\n",
       "  0.02975105121731758,\n",
       "  0.02891653962433338,\n",
       "  0.029650457203388214,\n",
       "  0.030501406639814377,\n",
       "  0.03350858390331268,\n",
       "  0.03220553323626518,\n",
       "  0.033143118023872375,\n",
       "  0.029911819845438004,\n",
       "  0.0317566953599453,\n",
       "  0.028574446216225624,\n",
       "  0.02974868379533291,\n",
       "  0.030030230060219765,\n",
       "  0.029921220615506172,\n",
       "  0.02970629185438156,\n",
       "  0.029334306716918945,\n",
       "  0.03250548988580704,\n",
       "  0.03194867819547653,\n",
       "  0.032584380358457565,\n",
       "  0.03356127813458443,\n",
       "  0.03124445118010044,\n",
       "  0.032304488122463226,\n",
       "  0.02865181677043438,\n",
       "  0.030033884570002556,\n",
       "  0.028160836547613144,\n",
       "  0.029580840840935707,\n",
       "  0.03171999752521515,\n",
       "  0.030683837831020355,\n",
       "  0.03070722706615925,\n",
       "  0.030284734442830086,\n",
       "  0.02985147386789322,\n",
       "  0.028259912505745888,\n",
       "  0.030065931379795074,\n",
       "  0.03236386924982071,\n",
       "  0.03287602961063385,\n",
       "  0.0339510552585125,\n",
       "  0.02934926748275757,\n",
       "  0.030865198001265526,\n",
       "  0.02899814583361149,\n",
       "  0.027598360553383827,\n",
       "  0.027660345658659935,\n",
       "  0.030310502275824547,\n",
       "  0.03041910007596016,\n",
       "  0.030105771496891975,\n",
       "  0.02890951558947563,\n",
       "  0.027750998735427856,\n",
       "  0.028622055426239967,\n",
       "  0.02900761552155018,\n",
       "  0.02917192131280899,\n",
       "  0.02914520353078842,\n",
       "  0.030400142073631287,\n",
       "  0.02880982495844364,\n",
       "  0.028074804693460464,\n",
       "  0.027480248361825943,\n",
       "  0.028384605422616005,\n",
       "  0.028290828689932823,\n",
       "  0.028767555952072144,\n",
       "  0.0310668982565403,\n",
       "  0.028351951390504837,\n",
       "  0.028845038264989853,\n",
       "  0.032394807785749435,\n",
       "  0.029574712738394737,\n",
       "  0.029272038489580154,\n",
       "  0.02796032279729843,\n",
       "  0.03230791166424751,\n",
       "  0.03186019882559776,\n",
       "  0.03253244608640671,\n",
       "  0.031222697347402573,\n",
       "  0.028769582509994507,\n",
       "  0.02956371381878853,\n",
       "  0.032102230936288834,\n",
       "  0.03525775298476219,\n",
       "  0.029090022668242455,\n",
       "  0.028835883364081383,\n",
       "  0.02830585651099682,\n",
       "  0.0272403284907341,\n",
       "  0.028071753680706024,\n",
       "  0.027216466143727303,\n",
       "  0.029007626697421074,\n",
       "  0.02957397885620594,\n",
       "  0.02944795787334442,\n",
       "  0.02849528193473816,\n",
       "  0.030418965965509415,\n",
       "  0.028152961283922195,\n",
       "  0.027593854814767838,\n",
       "  0.027308426797389984,\n",
       "  0.02715826965868473,\n",
       "  0.028112810105085373,\n",
       "  0.02920101210474968,\n",
       "  0.0317804180085659,\n",
       "  0.027780335396528244,\n",
       "  0.02797854132950306,\n",
       "  0.029336504638195038,\n",
       "  0.029517553746700287,\n",
       "  0.036033451557159424,\n",
       "  0.03550997003912926,\n",
       "  0.0350550040602684,\n",
       "  0.03030247800052166,\n",
       "  0.02855061925947666,\n",
       "  0.027793778106570244,\n",
       "  0.0297531858086586,\n",
       "  0.029623163864016533,\n",
       "  0.032185107469558716,\n",
       "  0.03276081755757332,\n",
       "  0.029325125738978386,\n",
       "  0.02776852808892727,\n",
       "  0.027840880677103996,\n",
       "  0.028489217162132263,\n",
       "  0.02883562259376049,\n",
       "  0.02737894468009472,\n",
       "  0.030358552932739258,\n",
       "  0.029696227982640266,\n",
       "  0.02670653723180294,\n",
       "  0.0290356632322073,\n",
       "  0.029419690370559692,\n",
       "  0.033382102847099304,\n",
       "  0.03393162786960602,\n",
       "  0.030368415638804436,\n",
       "  0.035986706614494324,\n",
       "  0.029511140659451485,\n",
       "  0.029732538387179375,\n",
       "  0.03094521351158619,\n",
       "  0.030253976583480835,\n",
       "  0.027037687599658966,\n",
       "  0.027641013264656067,\n",
       "  0.029317352920770645,\n",
       "  0.028688501566648483,\n",
       "  0.027628479525446892,\n",
       "  0.027521608397364616,\n",
       "  0.027324363589286804,\n",
       "  0.026633726432919502,\n",
       "  0.028810542076826096,\n",
       "  0.030786950141191483,\n",
       "  0.029864907264709473,\n",
       "  0.028737306594848633,\n",
       "  0.031113401055336,\n",
       "  0.03508951514959335,\n",
       "  0.03736669570207596,\n",
       "  0.03814961016178131,\n",
       "  0.03480377048254013,\n",
       "  0.03181544318795204,\n",
       "  0.029235491529107094,\n",
       "  0.027017179876565933,\n",
       "  0.02765073999762535,\n",
       "  0.029863405972719193,\n",
       "  0.03184758126735687,\n",
       "  0.030500050634145737,\n",
       "  0.029011404141783714,\n",
       "  0.03139582276344299,\n",
       "  0.030648013576865196,\n",
       "  0.028788762167096138,\n",
       "  0.027150770649313927,\n",
       "  0.02691982127726078,\n",
       "  0.02948564663529396,\n",
       "  0.03364770859479904,\n",
       "  0.031412966549396515,\n",
       "  0.02918914332985878,\n",
       "  0.030092788860201836,\n",
       "  0.033594802021980286,\n",
       "  0.031703460961580276,\n",
       "  0.02916526421904564,\n",
       "  0.02896410971879959,\n",
       "  0.030727138742804527,\n",
       "  0.02897198684513569,\n",
       "  0.029774034395813942,\n",
       "  0.028554394841194153,\n",
       "  0.026942208409309387,\n",
       "  0.026759151369333267,\n",
       "  0.028613664209842682,\n",
       "  0.02995939739048481,\n",
       "  0.027811042964458466,\n",
       "  0.0276601854711771,\n",
       "  0.027453411370515823,\n",
       "  0.029002830386161804,\n",
       "  0.02791900373995304,\n",
       "  0.026593003422021866,\n",
       "  0.027436070144176483,\n",
       "  0.028597092255949974,\n",
       "  0.028371233493089676,\n",
       "  0.02800057828426361,\n",
       "  0.027224885299801826,\n",
       "  0.0267739687114954,\n",
       "  0.026674767956137657,\n",
       "  0.027851203456521034,\n",
       "  0.027085615321993828,\n",
       "  0.027953557670116425,\n",
       "  0.027142181992530823,\n",
       "  0.026813624426722527,\n",
       "  0.02734336256980896,\n",
       "  0.026419753208756447,\n",
       "  0.02634931355714798,\n",
       "  0.026729756966233253,\n",
       "  0.026796314865350723,\n",
       "  0.029062829911708832,\n",
       "  0.036065153777599335,\n",
       "  0.03273197263479233,\n",
       "  0.032016098499298096,\n",
       "  0.0381876640021801,\n",
       "  0.032406967133283615,\n",
       "  0.030503107234835625,\n",
       "  0.029178140684962273,\n",
       "  0.03735800459980965,\n",
       "  0.03497685492038727,\n",
       "  0.03566516935825348,\n",
       "  0.027314508333802223,\n",
       "  0.026396233588457108,\n",
       "  0.02746530808508396,\n",
       "  0.026644570752978325,\n",
       "  0.02572135254740715,\n",
       "  0.028773469850420952,\n",
       "  0.027378559112548828,\n",
       "  0.02820397913455963,\n",
       "  0.027512140572071075,\n",
       "  0.027243055403232574,\n",
       "  0.0273275189101696,\n",
       "  0.026266897097229958,\n",
       "  0.026488374918699265,\n",
       "  0.02708577923476696,\n",
       "  0.02626371756196022,\n",
       "  0.026022687554359436,\n",
       "  0.028365066275000572,\n",
       "  0.028320880606770515,\n",
       "  0.02711171656847,\n",
       "  0.0257717277854681,\n",
       "  0.025565946474671364,\n",
       "  0.02601100690662861,\n",
       "  0.026029853150248528,\n",
       "  0.026041999459266663,\n",
       "  0.02636374533176422,\n",
       "  0.026576627045869827,\n",
       "  0.02558487467467785,\n",
       "  0.02778480388224125,\n",
       "  0.026828086003661156,\n",
       "  0.026965484023094177,\n",
       "  0.028793787583708763,\n",
       "  0.029513422399759293,\n",
       "  0.028890032321214676,\n",
       "  0.029004985466599464,\n",
       "  0.0277907382696867,\n",
       "  0.025962039828300476,\n",
       "  0.02640714682638645,\n",
       "  0.026355572044849396,\n",
       "  0.025490975007414818,\n",
       "  0.026931755244731903,\n",
       "  0.025955161079764366,\n",
       "  0.028467917814850807,\n",
       "  0.026854176074266434,\n",
       "  0.025576306506991386,\n",
       "  0.02703685872256756,\n",
       "  0.02534167468547821,\n",
       "  0.02694452740252018,\n",
       "  0.029525151476264,\n",
       "  0.02595602348446846,\n",
       "  0.02682030014693737,\n",
       "  0.025744857266545296,\n",
       "  0.026563407853245735,\n",
       "  0.030446412041783333,\n",
       "  0.028790656477212906,\n",
       "  0.025647716596722603,\n",
       "  0.026419417932629585,\n",
       "  0.03422139957547188,\n",
       "  0.037600405514240265,\n",
       "  0.029671868309378624,\n",
       "  0.027977708727121353,\n",
       "  0.02523135393857956,\n",
       "  0.026653362438082695,\n",
       "  0.027680065482854843,\n",
       "  0.030101129785180092,\n",
       "  0.03059486486017704,\n",
       "  0.02885865420103073,\n",
       "  0.026933731511235237,\n",
       "  0.028251320123672485,\n",
       "  0.025263622403144836,\n",
       "  0.02497529797255993,\n",
       "  0.02603057026863098,\n",
       "  0.02698579803109169,\n",
       "  0.027635136619210243,\n",
       "  0.03035232052206993,\n",
       "  0.031538400799036026,\n",
       "  0.026600968092679977,\n",
       "  0.02898496203124523,\n",
       "  0.027936922386288643,\n",
       "  0.026767771691083908,\n",
       "  0.02677125297486782,\n",
       "  0.029024887830018997,\n",
       "  0.025341607630252838,\n",
       "  0.025558339431881905,\n",
       "  0.028553282842040062,\n",
       "  0.026623954996466637,\n",
       "  0.025037402287125587,\n",
       "  0.026214882731437683,\n",
       "  0.027701914310455322,\n",
       "  0.031227970495820045,\n",
       "  0.025919059291481972,\n",
       "  0.025515057146549225,\n",
       "  0.024988139048218727,\n",
       "  0.02567174844443798,\n",
       "  0.024846287444233894,\n",
       "  0.02524138055741787,\n",
       "  0.027814488857984543,\n",
       "  0.027163874357938766,\n",
       "  0.025727398693561554,\n",
       "  0.025531817227602005,\n",
       "  0.025104809552431107,\n",
       "  0.026960337534546852,\n",
       "  0.029642201960086823,\n",
       "  0.027661239728331566,\n",
       "  0.025262683629989624,\n",
       "  0.025350801646709442,\n",
       "  0.025582673028111458,\n",
       "  0.025496363639831543,\n",
       "  0.027444584295153618,\n",
       "  0.025670204311609268,\n",
       "  0.030179893597960472,\n",
       "  0.029076728969812393,\n",
       "  0.028404995799064636,\n",
       "  0.024728110060095787,\n",
       "  0.02609548531472683,\n",
       "  0.026140686124563217,\n",
       "  0.031546592712402344,\n",
       "  0.029358243569731712,\n",
       "  0.032488126307725906,\n",
       "  0.0240202397108078,\n",
       "  0.024886084720492363,\n",
       "  0.025704849511384964,\n",
       "  0.02933911234140396,\n",
       "  0.02418377809226513,\n",
       "  0.025696996599435806,\n",
       "  0.025899890810251236,\n",
       "  0.023609111085534096,\n",
       "  0.027494054287672043,\n",
       "  0.036463212221860886,\n",
       "  0.029186343774199486,\n",
       "  0.025641251355409622,\n",
       "  0.025830352678894997,\n",
       "  0.02611844800412655,\n",
       "  0.02626536227762699,\n",
       "  0.02587122842669487,\n",
       "  0.026550903916358948,\n",
       "  0.02968275174498558,\n",
       "  0.027448391541838646,\n",
       "  0.02614002116024494,\n",
       "  0.023767825216054916,\n",
       "  0.025614876300096512,\n",
       "  0.026941362768411636,\n",
       "  0.024480387568473816,\n",
       "  0.030205879360437393,\n",
       "  0.027422480285167694,\n",
       "  0.02790343575179577,\n",
       "  0.0301126167178154,\n",
       "  0.030456170439720154,\n",
       "  0.03048641048371792,\n",
       "  0.025981329381465912,\n",
       "  0.02520311065018177,\n",
       "  0.02419256418943405,\n",
       "  0.02720002457499504,\n",
       "  0.02588582970201969,\n",
       "  0.02749454416334629,\n",
       "  0.027827786281704903,\n",
       "  0.02539755590260029,\n",
       "  0.02502620220184326,\n",
       "  0.02554682083427906,\n",
       "  0.026570387184619904,\n",
       "  0.025807896628975868,\n",
       "  0.025113295763731003,\n",
       "  0.028300680220127106,\n",
       "  0.02836564928293228,\n",
       "  0.02784918062388897,\n",
       "  0.026962371543049812,\n",
       "  0.02692747861146927,\n",
       "  0.025308238342404366,\n",
       "  0.024653181433677673,\n",
       "  0.02417333982884884,\n",
       "  0.024895770475268364,\n",
       "  0.025161223486065865,\n",
       "  0.02406409941613674,\n",
       "  0.02566133439540863,\n",
       "  0.026852838695049286,\n",
       "  0.024183064699172974,\n",
       "  0.028503041714429855,\n",
       "  0.02630603313446045,\n",
       "  0.0267080869525671,\n",
       "  0.02902153506875038,\n",
       "  0.02456595189869404,\n",
       "  0.025424007326364517,\n",
       "  0.02774604968726635,\n",
       "  0.025114744901657104,\n",
       "  0.024478448554873466,\n",
       "  0.02435341291129589,\n",
       "  0.025560371577739716,\n",
       "  0.023659545928239822,\n",
       "  0.026307864114642143,\n",
       "  0.03382788971066475,\n",
       "  0.03163929656147957,\n",
       "  0.0252920463681221,\n",
       "  0.025039535015821457,\n",
       "  0.025694841518998146,\n",
       "  0.025356987491250038,\n",
       "  0.02410798706114292,\n",
       "  0.02603345364332199,\n",
       "  0.024639811366796494,\n",
       "  0.027619948610663414,\n",
       "  0.027103299275040627,\n",
       "  0.025708351284265518,\n",
       "  0.024551739916205406,\n",
       "  0.025949185714125633,\n",
       "  0.025910871103405952,\n",
       "  0.02674107998609543,\n",
       "  0.023435980081558228,\n",
       "  0.025223461911082268,\n",
       "  0.023812834173440933,\n",
       "  0.02769496664404869,\n",
       "  0.024763546884059906,\n",
       "  0.027826152741909027,\n",
       "  0.025698455050587654,\n",
       "  0.02430175431072712,\n",
       "  0.024889981374144554,\n",
       "  0.026189230382442474,\n",
       "  0.0273025743663311,\n",
       "  0.02383291721343994,\n",
       "  0.024513257667422295,\n",
       "  0.02297130413353443,\n",
       "  0.02638501115143299,\n",
       "  0.029423348605632782,\n",
       "  0.024745961651206017,\n",
       "  0.025836320593953133,\n",
       "  0.027973731979727745,\n",
       "  0.030810987576842308,\n",
       "  0.030761944130063057,\n",
       "  0.025793319568037987,\n",
       "  0.024515800178050995,\n",
       "  0.0245355311781168,\n",
       "  0.0275732334703207,\n",
       "  0.024005362764000893,\n",
       "  0.023435067385435104,\n",
       "  0.023755837231874466,\n",
       "  0.024232808500528336,\n",
       "  0.030395234003663063,\n",
       "  0.032789479941129684,\n",
       "  0.027728118002414703,\n",
       "  0.024664664641022682,\n",
       "  0.025368481874465942,\n",
       "  0.024637063965201378,\n",
       "  0.023830067366361618,\n",
       "  0.025186002254486084,\n",
       "  0.025805575773119926,\n",
       "  0.02472604438662529,\n",
       "  0.026336943730711937,\n",
       "  0.026974515989422798,\n",
       "  0.03300900012254715,\n",
       "  0.029628511518239975,\n",
       "  0.02716990001499653,\n",
       "  0.02662542276084423,\n",
       "  0.02694777026772499,\n",
       "  0.025430943816900253,\n",
       "  0.025606775656342506,\n",
       "  0.023561209440231323,\n",
       "  0.02385755628347397,\n",
       "  0.023516647517681122,\n",
       "  0.023397741839289665,\n",
       "  0.025386100634932518,\n",
       "  0.024169202893972397,\n",
       "  0.023490186780691147,\n",
       "  0.02352425642311573,\n",
       "  0.031921762973070145,\n",
       "  0.034400876611471176,\n",
       "  0.03437463566660881,\n",
       "  0.028410954400897026,\n",
       "  0.02830362133681774,\n",
       "  0.02730111964046955,\n",
       "  0.025804812088608742,\n",
       "  0.026288289576768875,\n",
       "  0.025850169360637665,\n",
       "  0.025973880663514137,\n",
       "  0.027284899726510048,\n",
       "  0.023344071581959724,\n",
       "  0.02528977021574974,\n",
       "  0.022985896095633507,\n",
       "  0.023228084668517113,\n",
       "  ...],\n",
       " 'val_accuracy': [0.8700000047683716,\n",
       "  0.881538450717926,\n",
       "  0.9046154022216797,\n",
       "  0.9130769371986389,\n",
       "  0.9169231057167053,\n",
       "  0.920769214630127,\n",
       "  0.9215384721755981,\n",
       "  0.9223076701164246,\n",
       "  0.9215384721755981,\n",
       "  0.9253846406936646,\n",
       "  0.9253846406936646,\n",
       "  0.9246153831481934,\n",
       "  0.9276922941207886,\n",
       "  0.9292307496070862,\n",
       "  0.9284615516662598,\n",
       "  0.9292307496070862,\n",
       "  0.9284615516662598,\n",
       "  0.9300000071525574,\n",
       "  0.9323077201843262,\n",
       "  0.9307692050933838,\n",
       "  0.9323077201843262,\n",
       "  0.9338461756706238,\n",
       "  0.9323077201843262,\n",
       "  0.9376922845840454,\n",
       "  0.939230740070343,\n",
       "  0.9399999976158142,\n",
       "  0.9399999976158142,\n",
       "  0.9438461661338806,\n",
       "  0.942307710647583,\n",
       "  0.9399999976158142,\n",
       "  0.9430769085884094,\n",
       "  0.9453846216201782,\n",
       "  0.949999988079071,\n",
       "  0.9515384435653687,\n",
       "  0.9453846216201782,\n",
       "  0.9469230771064758,\n",
       "  0.949999988079071,\n",
       "  0.9492307901382446,\n",
       "  0.9515384435653687,\n",
       "  0.9492307901382446,\n",
       "  0.9515384435653687,\n",
       "  0.9461538195610046,\n",
       "  0.9561538696289062,\n",
       "  0.9607692360877991,\n",
       "  0.9630769491195679,\n",
       "  0.9546154141426086,\n",
       "  0.947692334651947,\n",
       "  0.9492307901382446,\n",
       "  0.9623076915740967,\n",
       "  0.9615384340286255,\n",
       "  0.9630769491195679,\n",
       "  0.9607692360877991,\n",
       "  0.9615384340286255,\n",
       "  0.9523077011108398,\n",
       "  0.9553846120834351,\n",
       "  0.9599999785423279,\n",
       "  0.9661538600921631,\n",
       "  0.9646154046058655,\n",
       "  0.9646154046058655,\n",
       "  0.9661538600921631,\n",
       "  0.9669230580329895,\n",
       "  0.9646154046058655,\n",
       "  0.9599999785423279,\n",
       "  0.9661538600921631,\n",
       "  0.9661538600921631,\n",
       "  0.9676923155784607,\n",
       "  0.9646154046058655,\n",
       "  0.9661538600921631,\n",
       "  0.9707692265510559,\n",
       "  0.9684615135192871,\n",
       "  0.9707692265510559,\n",
       "  0.9692307710647583,\n",
       "  0.9700000286102295,\n",
       "  0.9707692265510559,\n",
       "  0.9707692265510559,\n",
       "  0.9723076820373535,\n",
       "  0.9738461375236511,\n",
       "  0.9723076820373535,\n",
       "  0.9723076820373535,\n",
       "  0.9715384840965271,\n",
       "  0.9676923155784607,\n",
       "  0.9753845930099487,\n",
       "  0.9746153950691223,\n",
       "  0.9730769395828247,\n",
       "  0.9753845930099487,\n",
       "  0.9723076820373535,\n",
       "  0.9730769395828247,\n",
       "  0.9761538505554199,\n",
       "  0.9738461375236511,\n",
       "  0.9723076820373535,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9730769395828247,\n",
       "  0.9746153950691223,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9761538505554199,\n",
       "  0.9738461375236511,\n",
       "  0.9769230484962463,\n",
       "  0.9730769395828247,\n",
       "  0.9723076820373535,\n",
       "  0.9761538505554199,\n",
       "  0.9784615635871887,\n",
       "  0.9761538505554199,\n",
       "  0.9784615635871887,\n",
       "  0.9800000190734863,\n",
       "  0.9769230484962463,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9800000190734863,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9784615635871887,\n",
       "  0.9769230484962463,\n",
       "  0.9723076820373535,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9807692170143127,\n",
       "  0.9807692170143127,\n",
       "  0.9746153950691223,\n",
       "  0.9753845930099487,\n",
       "  0.9800000190734863,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9807692170143127,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9792307615280151,\n",
       "  0.9823076725006104,\n",
       "  0.9800000190734863,\n",
       "  0.9784615635871887,\n",
       "  0.9769230484962463,\n",
       "  0.9776923060417175,\n",
       "  0.9792307615280151,\n",
       "  0.9800000190734863,\n",
       "  0.9807692170143127,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9807692170143127,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9800000190734863,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9792307615280151,\n",
       "  0.9823076725006104,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9807692170143127,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9800000190734863,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9823076725006104,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9800000190734863,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9792307615280151,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9815384745597839,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9823076725006104,\n",
       "  0.9746153950691223,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9800000190734863,\n",
       "  0.9684615135192871,\n",
       "  0.9769230484962463,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.986923098564148,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9730769395828247,\n",
       "  0.9723076820373535,\n",
       "  0.9815384745597839,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9792307615280151,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9761538505554199,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9807692170143127,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9815384745597839,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9846153855323792,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9815384745597839,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9761538505554199,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9807692170143127,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9823076725006104,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  ...],\n",
       " 'val_loss': [0.3160666227340698,\n",
       "  0.32000431418418884,\n",
       "  0.29555097222328186,\n",
       "  0.27126577496528625,\n",
       "  0.2510669231414795,\n",
       "  0.23922811448574066,\n",
       "  0.23320746421813965,\n",
       "  0.22675666213035583,\n",
       "  0.2212020605802536,\n",
       "  0.217363640666008,\n",
       "  0.21421797573566437,\n",
       "  0.21092453598976135,\n",
       "  0.2082521915435791,\n",
       "  0.20445100963115692,\n",
       "  0.20180165767669678,\n",
       "  0.20004920661449432,\n",
       "  0.19560734927654266,\n",
       "  0.1917455792427063,\n",
       "  0.18977993726730347,\n",
       "  0.18632178008556366,\n",
       "  0.18332672119140625,\n",
       "  0.18034599721431732,\n",
       "  0.17751437425613403,\n",
       "  0.17532920837402344,\n",
       "  0.17177341878414154,\n",
       "  0.16893194615840912,\n",
       "  0.1648644357919693,\n",
       "  0.16256467998027802,\n",
       "  0.15844552218914032,\n",
       "  0.1548261195421219,\n",
       "  0.15181736648082733,\n",
       "  0.14749552309513092,\n",
       "  0.14513538777828217,\n",
       "  0.1439424753189087,\n",
       "  0.13821952044963837,\n",
       "  0.13578522205352783,\n",
       "  0.13436666131019592,\n",
       "  0.13245631754398346,\n",
       "  0.13259796798229218,\n",
       "  0.12800760567188263,\n",
       "  0.12619006633758545,\n",
       "  0.12527011334896088,\n",
       "  0.12509797513484955,\n",
       "  0.12424834072589874,\n",
       "  0.12632258236408234,\n",
       "  0.11959240585565567,\n",
       "  0.11865727603435516,\n",
       "  0.1191733106970787,\n",
       "  0.1165655180811882,\n",
       "  0.11540757119655609,\n",
       "  0.11389421671628952,\n",
       "  0.11273319274187088,\n",
       "  0.11069272458553314,\n",
       "  0.1137198805809021,\n",
       "  0.11003288626670837,\n",
       "  0.10720386356115341,\n",
       "  0.11090481281280518,\n",
       "  0.115956611931324,\n",
       "  0.10640986263751984,\n",
       "  0.10603209584951401,\n",
       "  0.11438202857971191,\n",
       "  0.10258986055850983,\n",
       "  0.10406908392906189,\n",
       "  0.10036003589630127,\n",
       "  0.10977520048618317,\n",
       "  0.10262531042098999,\n",
       "  0.09777828305959702,\n",
       "  0.09631264954805374,\n",
       "  0.09533935785293579,\n",
       "  0.09690481424331665,\n",
       "  0.09513606131076813,\n",
       "  0.09322832524776459,\n",
       "  0.09470994770526886,\n",
       "  0.0942189171910286,\n",
       "  0.09253333508968353,\n",
       "  0.09214479476213455,\n",
       "  0.09043341875076294,\n",
       "  0.08821862936019897,\n",
       "  0.08716660737991333,\n",
       "  0.08709139376878738,\n",
       "  0.0882425382733345,\n",
       "  0.08717219531536102,\n",
       "  0.08505700528621674,\n",
       "  0.08481595665216446,\n",
       "  0.08391671627759933,\n",
       "  0.08329079300165176,\n",
       "  0.08227092027664185,\n",
       "  0.08192867040634155,\n",
       "  0.08092387020587921,\n",
       "  0.0813831239938736,\n",
       "  0.08107976615428925,\n",
       "  0.08005671203136444,\n",
       "  0.07868007570505142,\n",
       "  0.07778331637382507,\n",
       "  0.07848899066448212,\n",
       "  0.07772940397262573,\n",
       "  0.07618149369955063,\n",
       "  0.0783219188451767,\n",
       "  0.07502791285514832,\n",
       "  0.07704413682222366,\n",
       "  0.08065751940011978,\n",
       "  0.07445546984672546,\n",
       "  0.07418131083250046,\n",
       "  0.07386142760515213,\n",
       "  0.07219454646110535,\n",
       "  0.07201336324214935,\n",
       "  0.071585513651371,\n",
       "  0.07019787281751633,\n",
       "  0.0697641670703888,\n",
       "  0.06934273988008499,\n",
       "  0.072044737637043,\n",
       "  0.07080785930156708,\n",
       "  0.07048378884792328,\n",
       "  0.06791307777166367,\n",
       "  0.06872957199811935,\n",
       "  0.07927019894123077,\n",
       "  0.07146605849266052,\n",
       "  0.06686341762542725,\n",
       "  0.06617625802755356,\n",
       "  0.06824978440999985,\n",
       "  0.07165107131004333,\n",
       "  0.06842757761478424,\n",
       "  0.06629853695631027,\n",
       "  0.06601070612668991,\n",
       "  0.06685476750135422,\n",
       "  0.07497625797986984,\n",
       "  0.07447371631860733,\n",
       "  0.06505204737186432,\n",
       "  0.06469877064228058,\n",
       "  0.06432170420885086,\n",
       "  0.06410253793001175,\n",
       "  0.06536687910556793,\n",
       "  0.06880088150501251,\n",
       "  0.0645914152264595,\n",
       "  0.06340154260396957,\n",
       "  0.06439485400915146,\n",
       "  0.06284759193658829,\n",
       "  0.06615181267261505,\n",
       "  0.0661444216966629,\n",
       "  0.06554710865020752,\n",
       "  0.06406788527965546,\n",
       "  0.06596653908491135,\n",
       "  0.06161053106188774,\n",
       "  0.06482376158237457,\n",
       "  0.0614410862326622,\n",
       "  0.06074465066194534,\n",
       "  0.06161028519272804,\n",
       "  0.06120023876428604,\n",
       "  0.062383152544498444,\n",
       "  0.060380689799785614,\n",
       "  0.06048684939742088,\n",
       "  0.05981314927339554,\n",
       "  0.060122404247522354,\n",
       "  0.060361314564943314,\n",
       "  0.06577689945697784,\n",
       "  0.06122850999236107,\n",
       "  0.06211939826607704,\n",
       "  0.059814196079969406,\n",
       "  0.05969562381505966,\n",
       "  0.0669347420334816,\n",
       "  0.06445109844207764,\n",
       "  0.06909166276454926,\n",
       "  0.061752740293741226,\n",
       "  0.06425239890813828,\n",
       "  0.06854400783777237,\n",
       "  0.06632046401500702,\n",
       "  0.060456130653619766,\n",
       "  0.05951308086514473,\n",
       "  0.05911574140191078,\n",
       "  0.05904319882392883,\n",
       "  0.06018202006816864,\n",
       "  0.05945243313908577,\n",
       "  0.05892588570713997,\n",
       "  0.05878443270921707,\n",
       "  0.06215565651655197,\n",
       "  0.0601608082652092,\n",
       "  0.05908415466547012,\n",
       "  0.05858317390084267,\n",
       "  0.058815035969018936,\n",
       "  0.05911467969417572,\n",
       "  0.058742642402648926,\n",
       "  0.05870147794485092,\n",
       "  0.058630865067243576,\n",
       "  0.05869762599468231,\n",
       "  0.061717987060546875,\n",
       "  0.05925637483596802,\n",
       "  0.06018153205513954,\n",
       "  0.05878088250756264,\n",
       "  0.05851513147354126,\n",
       "  0.05862913280725479,\n",
       "  0.06206626445055008,\n",
       "  0.06254477798938751,\n",
       "  0.05883108824491501,\n",
       "  0.05818544328212738,\n",
       "  0.06088549271225929,\n",
       "  0.06054176762700081,\n",
       "  0.0606282502412796,\n",
       "  0.05924627557396889,\n",
       "  0.0607786551117897,\n",
       "  0.05997540429234505,\n",
       "  0.06128334626555443,\n",
       "  0.05876774713397026,\n",
       "  0.058013781905174255,\n",
       "  0.057338085025548935,\n",
       "  0.05728783831000328,\n",
       "  0.05699201300740242,\n",
       "  0.05726083740592003,\n",
       "  0.059282559901475906,\n",
       "  0.059502311050891876,\n",
       "  0.05931257829070091,\n",
       "  0.0594443678855896,\n",
       "  0.056812286376953125,\n",
       "  0.056645579636096954,\n",
       "  0.057113248854875565,\n",
       "  0.05674225464463234,\n",
       "  0.058715660125017166,\n",
       "  0.05974963307380676,\n",
       "  0.05690108239650726,\n",
       "  0.056834328919649124,\n",
       "  0.05723811313509941,\n",
       "  0.056667573750019073,\n",
       "  0.05724320188164711,\n",
       "  0.058262962847948074,\n",
       "  0.05644810199737549,\n",
       "  0.055504996329545975,\n",
       "  0.05532349646091461,\n",
       "  0.05559865012764931,\n",
       "  0.05916960537433624,\n",
       "  0.06062928959727287,\n",
       "  0.05687045305967331,\n",
       "  0.058876264840364456,\n",
       "  0.056008558720350266,\n",
       "  0.05537058785557747,\n",
       "  0.05573716387152672,\n",
       "  0.056739870458841324,\n",
       "  0.05550147593021393,\n",
       "  0.05548473820090294,\n",
       "  0.05541123077273369,\n",
       "  0.05555246025323868,\n",
       "  0.05586477369070053,\n",
       "  0.05572576820850372,\n",
       "  0.05579548329114914,\n",
       "  0.0565163791179657,\n",
       "  0.05905722826719284,\n",
       "  0.05554578825831413,\n",
       "  0.055688921362161636,\n",
       "  0.057487234473228455,\n",
       "  0.05867890641093254,\n",
       "  0.05515579134225845,\n",
       "  0.05533130466938019,\n",
       "  0.0549781396985054,\n",
       "  0.061687592417001724,\n",
       "  0.05703767016530037,\n",
       "  0.05671527981758118,\n",
       "  0.057586751878261566,\n",
       "  0.06248690187931061,\n",
       "  0.05516478419303894,\n",
       "  0.05384056270122528,\n",
       "  0.05512762814760208,\n",
       "  0.05514572560787201,\n",
       "  0.05519220978021622,\n",
       "  0.0545697957277298,\n",
       "  0.059817858040332794,\n",
       "  0.06002625823020935,\n",
       "  0.060902662575244904,\n",
       "  0.061806969344615936,\n",
       "  0.055228278040885925,\n",
       "  0.059238553047180176,\n",
       "  0.05471378564834595,\n",
       "  0.05639851465821266,\n",
       "  0.05452181398868561,\n",
       "  0.05466873571276665,\n",
       "  0.05461454391479492,\n",
       "  0.054623741656541824,\n",
       "  0.05683564394712448,\n",
       "  0.05486688390374184,\n",
       "  0.060910798609256744,\n",
       "  0.05546143278479576,\n",
       "  0.053824253380298615,\n",
       "  0.05510998144745827,\n",
       "  0.05467560887336731,\n",
       "  0.061267830431461334,\n",
       "  0.06098353862762451,\n",
       "  0.05645231902599335,\n",
       "  0.058511558920145035,\n",
       "  0.05755937471985817,\n",
       "  0.05399399995803833,\n",
       "  0.05511187016963959,\n",
       "  0.05578844994306564,\n",
       "  0.056743163615465164,\n",
       "  0.05423638969659805,\n",
       "  0.06192420423030853,\n",
       "  0.05534333363175392,\n",
       "  0.05635754391551018,\n",
       "  0.05661969631910324,\n",
       "  0.05652257055044174,\n",
       "  0.06140744313597679,\n",
       "  0.05552839860320091,\n",
       "  0.054966848343610764,\n",
       "  0.05506090447306633,\n",
       "  0.054037727415561676,\n",
       "  0.054501812905073166,\n",
       "  0.05757933482527733,\n",
       "  0.06185179576277733,\n",
       "  0.07563897967338562,\n",
       "  0.053718771785497665,\n",
       "  0.056664470583200455,\n",
       "  0.05532414838671684,\n",
       "  0.05517930909991264,\n",
       "  0.057460445910692215,\n",
       "  0.058127470314502716,\n",
       "  0.053556643426418304,\n",
       "  0.05495384708046913,\n",
       "  0.053386762738227844,\n",
       "  0.05300862342119217,\n",
       "  0.05694010853767395,\n",
       "  0.0536075122654438,\n",
       "  0.05444702133536339,\n",
       "  0.053438615053892136,\n",
       "  0.05396364629268646,\n",
       "  0.05336742848157883,\n",
       "  0.05543898418545723,\n",
       "  0.055910222232341766,\n",
       "  0.05754953250288963,\n",
       "  0.05662624537944794,\n",
       "  0.05675102397799492,\n",
       "  0.053117088973522186,\n",
       "  0.05537150427699089,\n",
       "  0.055828891694545746,\n",
       "  0.05348999425768852,\n",
       "  0.056751932948827744,\n",
       "  0.07845983654260635,\n",
       "  0.058190226554870605,\n",
       "  0.05877097323536873,\n",
       "  0.05404723063111305,\n",
       "  0.05388694629073143,\n",
       "  0.0533120334148407,\n",
       "  0.05319209024310112,\n",
       "  0.05453629791736603,\n",
       "  0.05408361554145813,\n",
       "  0.05433310940861702,\n",
       "  0.05469917878508568,\n",
       "  0.053199585527181625,\n",
       "  0.053423937410116196,\n",
       "  0.05602350831031799,\n",
       "  0.0532110370695591,\n",
       "  0.05347118526697159,\n",
       "  0.054142214357852936,\n",
       "  0.0538552850484848,\n",
       "  0.056542616337537766,\n",
       "  0.05494428798556328,\n",
       "  0.055897217243909836,\n",
       "  0.05650992691516876,\n",
       "  0.06431383639574051,\n",
       "  0.06859749555587769,\n",
       "  0.09786820411682129,\n",
       "  0.07068958878517151,\n",
       "  0.05851877108216286,\n",
       "  0.05347520112991333,\n",
       "  0.05303576961159706,\n",
       "  0.05477506294846535,\n",
       "  0.054775919765233994,\n",
       "  0.052887462079524994,\n",
       "  0.05505941063165665,\n",
       "  0.05403527989983559,\n",
       "  0.05347522348165512,\n",
       "  0.0524849072098732,\n",
       "  0.052446868270635605,\n",
       "  0.05840727686882019,\n",
       "  0.052977822721004486,\n",
       "  0.05235164985060692,\n",
       "  0.05347844585776329,\n",
       "  0.055374015122652054,\n",
       "  0.052710119634866714,\n",
       "  0.054217200726270676,\n",
       "  0.05351227521896362,\n",
       "  0.05378975346684456,\n",
       "  0.05260507017374039,\n",
       "  0.05272367596626282,\n",
       "  0.05352645367383957,\n",
       "  0.05389786511659622,\n",
       "  0.05272585526108742,\n",
       "  0.053121261298656464,\n",
       "  0.05877482891082764,\n",
       "  0.06200466305017471,\n",
       "  0.05341779440641403,\n",
       "  0.05337316542863846,\n",
       "  0.054972030222415924,\n",
       "  0.053797055035829544,\n",
       "  0.053774889558553696,\n",
       "  0.053512949496507645,\n",
       "  0.05352756381034851,\n",
       "  0.05286378785967827,\n",
       "  0.05247103050351143,\n",
       "  0.05285779386758804,\n",
       "  0.05344819277524948,\n",
       "  0.05363905057311058,\n",
       "  0.05309874191880226,\n",
       "  0.052357859909534454,\n",
       "  0.05568458512425423,\n",
       "  0.053552109748125076,\n",
       "  0.05245060846209526,\n",
       "  0.053018201142549515,\n",
       "  0.05285412073135376,\n",
       "  0.05351271480321884,\n",
       "  0.052885960787534714,\n",
       "  0.052409809082746506,\n",
       "  0.05268428474664688,\n",
       "  0.05355451628565788,\n",
       "  0.053180400282144547,\n",
       "  0.0540754534304142,\n",
       "  0.053545426577329636,\n",
       "  0.05592624470591545,\n",
       "  0.05221984162926674,\n",
       "  0.05689455568790436,\n",
       "  0.058796197175979614,\n",
       "  0.053822923451662064,\n",
       "  0.053819380700588226,\n",
       "  0.05349254980683327,\n",
       "  0.05264624208211899,\n",
       "  0.05333622172474861,\n",
       "  0.0553215891122818,\n",
       "  0.05551614612340927,\n",
       "  0.05353692173957825,\n",
       "  0.05397350713610649,\n",
       "  0.05815962702035904,\n",
       "  0.05599963292479515,\n",
       "  0.05625408887863159,\n",
       "  0.05468180775642395,\n",
       "  0.05213253200054169,\n",
       "  0.05634434148669243,\n",
       "  0.08588870614767075,\n",
       "  0.07956854999065399,\n",
       "  0.061203837394714355,\n",
       "  0.0526077039539814,\n",
       "  0.05331180989742279,\n",
       "  0.05320189893245697,\n",
       "  0.05343012884259224,\n",
       "  0.05245170742273331,\n",
       "  0.05299950763583183,\n",
       "  0.053403496742248535,\n",
       "  0.05435037985444069,\n",
       "  0.053311463445425034,\n",
       "  0.054518960416316986,\n",
       "  0.053905293345451355,\n",
       "  0.053444478660821915,\n",
       "  0.055257346481084824,\n",
       "  0.054063696414232254,\n",
       "  0.054606370627880096,\n",
       "  0.055498722940683365,\n",
       "  0.054462797939777374,\n",
       "  0.05472014844417572,\n",
       "  0.05338499695062637,\n",
       "  0.053248289972543716,\n",
       "  0.05506950244307518,\n",
       "  0.05452044680714607,\n",
       "  0.057373762130737305,\n",
       "  0.05446900427341461,\n",
       "  0.05459342524409294,\n",
       "  0.05780075117945671,\n",
       "  0.05410053953528404,\n",
       "  0.05461978167295456,\n",
       "  0.05507347732782364,\n",
       "  0.053303588181734085,\n",
       "  0.05440423637628555,\n",
       "  0.05421110987663269,\n",
       "  0.05411340668797493,\n",
       "  0.0547199547290802,\n",
       "  0.05467305704951286,\n",
       "  0.05607394874095917,\n",
       "  0.053517818450927734,\n",
       "  0.053571801632642746,\n",
       "  0.054985411465168,\n",
       "  0.05283530429005623,\n",
       "  0.05324926972389221,\n",
       "  0.055864367634058,\n",
       "  0.05408143997192383,\n",
       "  0.054715845733881,\n",
       "  0.05970798805356026,\n",
       "  0.0569242499768734,\n",
       "  0.0602041520178318,\n",
       "  0.06068842485547066,\n",
       "  0.055585138499736786,\n",
       "  0.05329334735870361,\n",
       "  0.05482974648475647,\n",
       "  0.05372897908091545,\n",
       "  0.05481009930372238,\n",
       "  0.05627340450882912,\n",
       "  0.06085299700498581,\n",
       "  0.05337439849972725,\n",
       "  0.056037768721580505,\n",
       "  0.053892336785793304,\n",
       "  0.05397842824459076,\n",
       "  0.053742147982120514,\n",
       "  0.05215974152088165,\n",
       "  0.05594063550233841,\n",
       "  0.05484720692038536,\n",
       "  0.06537880748510361,\n",
       "  0.0571274496614933,\n",
       "  0.05433661863207817,\n",
       "  0.06091301515698433,\n",
       "  0.05460277944803238,\n",
       "  0.05183624103665352,\n",
       "  0.055228084325790405,\n",
       "  0.05642472580075264,\n",
       "  0.05466335639357567,\n",
       "  0.06478087604045868,\n",
       "  0.06980076432228088,\n",
       "  0.05718809738755226,\n",
       "  0.05403560400009155,\n",
       "  0.05434810370206833,\n",
       "  0.05477388948202133,\n",
       "  0.05613609775900841,\n",
       "  0.05358048528432846,\n",
       "  0.05343964695930481,\n",
       "  0.05552084743976593,\n",
       "  0.05389299988746643,\n",
       "  0.06032994017004967,\n",
       "  0.056983161717653275,\n",
       "  0.05445301532745361,\n",
       "  0.05395014211535454,\n",
       "  0.05362839996814728,\n",
       "  0.054886672645807266,\n",
       "  0.052977293729782104,\n",
       "  0.055799126625061035,\n",
       "  0.0613933727145195,\n",
       "  0.05691169574856758,\n",
       "  0.055435214191675186,\n",
       "  0.05735831707715988,\n",
       "  0.05572162941098213,\n",
       "  0.05322688817977905,\n",
       "  0.0554540790617466,\n",
       "  0.05355033278465271,\n",
       "  0.0612662211060524,\n",
       "  0.05481074005365372,\n",
       "  0.05412651225924492,\n",
       "  0.061269309371709824,\n",
       "  0.06322765350341797,\n",
       "  0.05309148505330086,\n",
       "  0.06248407065868378,\n",
       "  0.0572998933494091,\n",
       "  0.05462164059281349,\n",
       "  0.05629001185297966,\n",
       "  0.05703555792570114,\n",
       "  0.05443957820534706,\n",
       "  0.05595247820019722,\n",
       "  0.056452278047800064,\n",
       "  0.05744912847876549,\n",
       "  0.05327877402305603,\n",
       "  0.053803276270627975,\n",
       "  0.05712511017918587,\n",
       "  0.05520807206630707,\n",
       "  0.05351356044411659,\n",
       "  0.05549144372344017,\n",
       "  0.0546877346932888,\n",
       "  0.05945391207933426,\n",
       "  0.057556457817554474,\n",
       "  0.05965891852974892,\n",
       "  0.056541137397289276,\n",
       "  0.05418364331126213,\n",
       "  0.05463158339262009,\n",
       "  0.056838132441043854,\n",
       "  0.05453598126769066,\n",
       "  0.05437162518501282,\n",
       "  0.054199639707803726,\n",
       "  0.05395425111055374,\n",
       "  0.05270634591579437,\n",
       "  0.054884057492017746,\n",
       "  0.055197201669216156,\n",
       "  0.05545183643698692,\n",
       "  0.054926201701164246,\n",
       "  0.056252218782901764,\n",
       "  0.05321098864078522,\n",
       "  0.054498568177223206,\n",
       "  0.05533616244792938,\n",
       "  0.05575306713581085,\n",
       "  0.05523915961384773,\n",
       "  0.056498412042856216,\n",
       "  0.05382014438509941,\n",
       "  0.053947512060403824,\n",
       "  0.05984164774417877,\n",
       "  0.057867150753736496,\n",
       "  0.05618946999311447,\n",
       "  0.05484328791499138,\n",
       "  0.056595463305711746,\n",
       "  0.05575154349207878,\n",
       "  0.06817058473825455,\n",
       "  0.06045081093907356,\n",
       "  0.0558769628405571,\n",
       "  0.053561095148324966,\n",
       "  0.054891496896743774,\n",
       "  0.056700561195611954,\n",
       "  0.056762389838695526,\n",
       "  0.05985769256949425,\n",
       "  0.05295562744140625,\n",
       "  0.053743086755275726,\n",
       "  0.05379738286137581,\n",
       "  0.052960146218538284,\n",
       "  0.054973602294921875,\n",
       "  0.05480900779366493,\n",
       "  0.05523519217967987,\n",
       "  0.05345882102847099,\n",
       "  0.06050843372941017,\n",
       "  0.055331725627183914,\n",
       "  0.05254665017127991,\n",
       "  0.054960090667009354,\n",
       "  0.05481807515025139,\n",
       "  0.0536784790456295,\n",
       "  0.05995608866214752,\n",
       "  0.05959094688296318,\n",
       "  0.05604976415634155,\n",
       "  0.05476522445678711,\n",
       "  0.054739680141210556,\n",
       "  0.05653940513730049,\n",
       "  0.06539217382669449,\n",
       "  0.06795432418584824,\n",
       "  0.06505636125802994,\n",
       "  0.059152305126190186,\n",
       "  0.05451510101556778,\n",
       "  0.05601714923977852,\n",
       "  0.05425785854458809,\n",
       "  0.05863180756568909,\n",
       "  0.054879430681467056,\n",
       "  0.055857568979263306,\n",
       "  0.05664420500397682,\n",
       "  0.056835949420928955,\n",
       "  0.05296148359775543,\n",
       "  0.05445413291454315,\n",
       "  0.057228006422519684,\n",
       "  0.054015304893255234,\n",
       "  0.053936392068862915,\n",
       "  0.06029241159558296,\n",
       "  0.05538066476583481,\n",
       "  0.0556284636259079,\n",
       "  0.05513967573642731,\n",
       "  0.05497369170188904,\n",
       "  0.06336815655231476,\n",
       "  0.05675966292619705,\n",
       "  0.06371229887008667,\n",
       "  0.05834156274795532,\n",
       "  0.055692847818136215,\n",
       "  0.060847532004117966,\n",
       "  0.06384231895208359,\n",
       "  0.053797561675310135,\n",
       "  0.055621590465307236,\n",
       "  0.054017968475818634,\n",
       "  0.05365664139389992,\n",
       "  0.05364206060767174,\n",
       "  0.055959537625312805,\n",
       "  0.05521504208445549,\n",
       "  0.05309449881315231,\n",
       "  0.058988071978092194,\n",
       "  0.056142307817935944,\n",
       "  0.054710980504751205,\n",
       "  0.0571744367480278,\n",
       "  0.05736469104886055,\n",
       "  0.06325912475585938,\n",
       "  0.06687436997890472,\n",
       "  0.05827169492840767,\n",
       "  0.0567486397922039,\n",
       "  0.05291099101305008,\n",
       "  0.06505265086889267,\n",
       "  0.051013488322496414,\n",
       "  0.05683249235153198,\n",
       "  0.0548502653837204,\n",
       "  0.05295794457197189,\n",
       "  0.05693184956908226,\n",
       "  0.05707213282585144,\n",
       "  0.06015472486615181,\n",
       "  0.05460457503795624,\n",
       "  0.0535116046667099,\n",
       "  0.052714649587869644,\n",
       "  0.05254082381725311,\n",
       "  0.05793971195816994,\n",
       "  0.053089845925569534,\n",
       "  0.06947056949138641,\n",
       "  0.054164640605449677,\n",
       "  0.05132070556282997,\n",
       "  0.06441298872232437,\n",
       "  0.06082463264465332,\n",
       "  0.05610736459493637,\n",
       "  0.053438086062669754,\n",
       "  0.05258083716034889,\n",
       "  0.05481163039803505,\n",
       "  0.05541694536805153,\n",
       "  0.056655723601579666,\n",
       "  0.05474313721060753,\n",
       "  0.054462358355522156,\n",
       "  0.05483904480934143,\n",
       "  0.0546705387532711,\n",
       "  0.05628468468785286,\n",
       "  0.053559910506010056,\n",
       "  0.05325606092810631,\n",
       "  0.05604710057377815,\n",
       "  0.05715970695018768,\n",
       "  0.05237646400928497,\n",
       "  0.05303030088543892,\n",
       "  0.055956028401851654,\n",
       "  0.05219760164618492,\n",
       "  0.05619841814041138,\n",
       "  0.05287931114435196,\n",
       "  0.05274856463074684,\n",
       "  0.05370268598198891,\n",
       "  0.05596055090427399,\n",
       "  0.05383547767996788,\n",
       "  0.05357133597135544,\n",
       "  0.05452100187540054,\n",
       "  0.052613403648138046,\n",
       "  0.05188562348484993,\n",
       "  0.05516707897186279,\n",
       "  0.0527532659471035,\n",
       "  0.053191814571619034,\n",
       "  0.052930284291505814,\n",
       "  0.05548960715532303,\n",
       "  0.06052067130804062,\n",
       "  0.05468059331178665,\n",
       "  0.05919002369046211,\n",
       "  0.0794469490647316,\n",
       "  0.06348361074924469,\n",
       "  0.05403883755207062,\n",
       "  0.054490432143211365,\n",
       "  0.0651097297668457,\n",
       "  0.06331714987754822,\n",
       "  0.060380637645721436,\n",
       "  0.054841987788677216,\n",
       "  0.055131904780864716,\n",
       "  0.054514605551958084,\n",
       "  0.052718665450811386,\n",
       "  0.054513316601514816,\n",
       "  0.059270523488521576,\n",
       "  0.05489494279026985,\n",
       "  0.052512090653181076,\n",
       "  0.056317038834095,\n",
       "  0.05358889698982239,\n",
       "  0.05527389049530029,\n",
       "  0.054753948003053665,\n",
       "  0.053850214928388596,\n",
       "  0.055237334221601486,\n",
       "  0.05297258868813515,\n",
       "  0.05300653725862503,\n",
       "  0.05919859558343887,\n",
       "  0.054267529398202896,\n",
       "  0.0536222942173481,\n",
       "  0.0540507510304451,\n",
       "  0.05313752219080925,\n",
       "  0.05291471630334854,\n",
       "  0.05335693806409836,\n",
       "  0.0555599145591259,\n",
       "  0.05603950098156929,\n",
       "  0.055763084441423416,\n",
       "  0.054366450756788254,\n",
       "  0.05750884488224983,\n",
       "  0.05406705290079117,\n",
       "  0.054797280579805374,\n",
       "  0.05626410245895386,\n",
       "  0.0620439387857914,\n",
       "  0.05781972408294678,\n",
       "  0.06085943430662155,\n",
       "  0.056884765625,\n",
       "  0.05392448976635933,\n",
       "  0.05565303936600685,\n",
       "  0.05565729737281799,\n",
       "  0.05440938100218773,\n",
       "  0.054124727845191956,\n",
       "  0.055195678025484085,\n",
       "  0.05319542437791824,\n",
       "  0.05388937145471573,\n",
       "  0.05485609918832779,\n",
       "  0.05659599229693413,\n",
       "  0.05362450331449509,\n",
       "  0.05452004820108414,\n",
       "  0.055125314742326736,\n",
       "  0.05683537572622299,\n",
       "  0.0581241212785244,\n",
       "  0.05521363392472267,\n",
       "  0.05395904555916786,\n",
       "  0.05326808616518974,\n",
       "  0.055312372744083405,\n",
       "  0.05371147021651268,\n",
       "  0.056137021631002426,\n",
       "  0.05917590856552124,\n",
       "  0.060533177107572556,\n",
       "  0.05407272279262543,\n",
       "  0.0544024258852005,\n",
       "  0.05420245975255966,\n",
       "  0.05537355691194534,\n",
       "  0.05770833417773247,\n",
       "  0.053266942501068115,\n",
       "  0.0554848313331604,\n",
       "  0.05979502201080322,\n",
       "  0.05715665966272354,\n",
       "  0.05739830434322357,\n",
       "  0.05531858652830124,\n",
       "  0.05414785072207451,\n",
       "  0.05345272272825241,\n",
       "  0.0546431802213192,\n",
       "  0.05478960648179054,\n",
       "  0.0536215677857399,\n",
       "  0.05995437130331993,\n",
       "  0.058186423033475876,\n",
       "  0.05696803331375122,\n",
       "  0.05525269731879234,\n",
       "  0.055136095732450485,\n",
       "  0.05701585114002228,\n",
       "  0.057707466185092926,\n",
       "  0.0528351366519928,\n",
       "  0.055363282561302185,\n",
       "  0.054919321089982986,\n",
       "  0.05322389304637909,\n",
       "  0.05417922884225845,\n",
       "  0.05467773601412773,\n",
       "  0.05295879766345024,\n",
       "  0.062147367745637894,\n",
       "  0.057164859026670456,\n",
       "  0.05436626821756363,\n",
       "  0.05371984466910362,\n",
       "  0.05537009611725807,\n",
       "  0.05466657131910324,\n",
       "  0.05352145433425903,\n",
       "  0.061150792986154556,\n",
       "  0.05674080550670624,\n",
       "  0.053704164922237396,\n",
       "  0.05423900485038757,\n",
       "  0.05579058825969696,\n",
       "  0.0544426366686821,\n",
       "  0.056529805064201355,\n",
       "  0.05729711800813675,\n",
       "  0.05436583235859871,\n",
       "  0.05302776023745537,\n",
       "  0.05542171001434326,\n",
       "  0.05693291872739792,\n",
       "  0.05680762603878975,\n",
       "  0.05657025799155235,\n",
       "  0.05954860895872116,\n",
       "  0.05836685001850128,\n",
       "  0.05381688475608826,\n",
       "  0.05653715133666992,\n",
       "  0.05571269616484642,\n",
       "  0.05614294856786728,\n",
       "  0.06235109269618988,\n",
       "  0.055707234889268875,\n",
       "  0.05365169048309326,\n",
       "  0.05578369274735451,\n",
       "  0.05488526076078415,\n",
       "  0.05257214605808258,\n",
       "  0.05824078992009163,\n",
       "  0.05325113236904144,\n",
       "  0.05281354486942291,\n",
       "  0.05679018050432205,\n",
       "  0.05343710631132126,\n",
       "  0.05657539889216423,\n",
       "  0.06375598907470703,\n",
       "  0.05842755362391472,\n",
       "  0.057104386389255524,\n",
       "  0.05800676345825195,\n",
       "  0.05311812087893486,\n",
       "  0.053784072399139404,\n",
       "  0.055248502641916275,\n",
       "  0.0533917061984539,\n",
       "  0.0619794987142086,\n",
       "  0.05796334892511368,\n",
       "  0.05249970778822899,\n",
       "  0.052717361599206924,\n",
       "  0.056732382625341415,\n",
       "  0.054691631346940994,\n",
       "  0.05257575586438179,\n",
       "  0.062439922243356705,\n",
       "  0.05673179030418396,\n",
       "  0.05415630713105202,\n",
       "  0.0558895505964756,\n",
       "  0.06828674674034119,\n",
       "  0.06472576409578323,\n",
       "  0.05482685938477516,\n",
       "  0.0558197945356369,\n",
       "  0.053599048405885696,\n",
       "  0.054728779941797256,\n",
       "  0.05982410907745361,\n",
       "  0.05438467115163803,\n",
       "  0.0563727505505085,\n",
       "  0.05440196022391319,\n",
       "  0.05645722150802612,\n",
       "  0.05392187088727951,\n",
       "  0.05310283228754997,\n",
       "  0.05798656865954399,\n",
       "  0.05344764143228531,\n",
       "  0.05406012386083603,\n",
       "  0.057159535586833954,\n",
       "  0.062029916793107986,\n",
       "  0.058603279292583466,\n",
       "  0.05560765042901039,\n",
       "  0.055123090744018555,\n",
       "  0.054430730640888214,\n",
       "  0.054343901574611664,\n",
       "  0.0557342953979969,\n",
       "  0.05420585721731186,\n",
       "  0.05417552590370178,\n",
       "  0.05636781454086304,\n",
       "  0.057915374636650085,\n",
       "  0.05656182020902634,\n",
       "  0.058464355766773224,\n",
       "  0.056858498603105545,\n",
       "  0.05637616664171219,\n",
       "  0.060552533715963364,\n",
       "  0.05528566986322403,\n",
       "  0.05300060659646988,\n",
       "  0.05467437580227852,\n",
       "  0.05516878515481949,\n",
       "  0.05196208506822586,\n",
       "  0.05572880804538727,\n",
       "  0.055364638566970825,\n",
       "  0.05473867058753967,\n",
       "  0.055729929357767105,\n",
       "  0.05979158729314804,\n",
       "  0.055054474622011185,\n",
       "  0.055167295038700104,\n",
       "  0.0588991604745388,\n",
       "  0.05891035869717598,\n",
       "  0.058489397168159485,\n",
       "  0.056511711329221725,\n",
       "  0.055363405495882034,\n",
       "  0.055090468376874924,\n",
       "  0.05744367092847824,\n",
       "  0.05926574766635895,\n",
       "  0.05610552430152893,\n",
       "  0.05483883246779442,\n",
       "  0.059113759547472,\n",
       "  0.056687939912080765,\n",
       "  0.060142140835523605,\n",
       "  0.05365318804979324,\n",
       "  0.059329524636268616,\n",
       "  0.053106214851140976,\n",
       "  0.060241829603910446,\n",
       "  0.05676581338047981,\n",
       "  0.054923687130212784,\n",
       "  0.053309354931116104,\n",
       "  0.055879347026348114,\n",
       "  0.05833597108721733,\n",
       "  0.05327797681093216,\n",
       "  0.06000732257962227,\n",
       "  0.055340882390737534,\n",
       "  0.0545448437333107,\n",
       "  0.05354494974017143,\n",
       "  0.05809945985674858,\n",
       "  0.05890008807182312,\n",
       "  0.05234832689166069,\n",
       "  0.05821419879794121,\n",
       "  0.05785385146737099,\n",
       "  0.05807233229279518,\n",
       "  0.07081672549247742,\n",
       "  0.056863751262426376,\n",
       "  0.05813237279653549,\n",
       "  0.055381547659635544,\n",
       "  0.0618281215429306,\n",
       "  0.05423570051789284,\n",
       "  0.05665045976638794,\n",
       "  0.05476076528429985,\n",
       "  0.052810657769441605,\n",
       "  0.05852764472365379,\n",
       "  0.0646880641579628,\n",
       "  0.053313616663217545,\n",
       "  0.05764806270599365,\n",
       "  0.05930726230144501,\n",
       "  0.054918281733989716,\n",
       "  0.05494718626141548,\n",
       "  0.05724853649735451,\n",
       "  0.05879872292280197,\n",
       "  0.055455006659030914,\n",
       "  0.05716251581907272,\n",
       "  0.060621436685323715,\n",
       "  0.07115674763917923,\n",
       "  0.05813309922814369,\n",
       "  0.05337411165237427,\n",
       "  0.0593322329223156,\n",
       "  0.06123623251914978,\n",
       "  0.05662956088781357,\n",
       "  0.06169281527400017,\n",
       "  0.056201741099357605,\n",
       "  0.05482042580842972,\n",
       "  0.05406847596168518,\n",
       "  0.054866086691617966,\n",
       "  0.05535218492150307,\n",
       "  0.05597540736198425,\n",
       "  0.054466646164655685,\n",
       "  0.05654780566692352,\n",
       "  0.05458009988069534,\n",
       "  0.06462446600198746,\n",
       "  0.07030294835567474,\n",
       "  0.0721960961818695,\n",
       "  0.05646195635199547,\n",
       "  0.05540822073817253,\n",
       "  0.05637068673968315,\n",
       "  0.061940278857946396,\n",
       "  0.05712907761335373,\n",
       "  0.05957711488008499,\n",
       "  0.0625363364815712,\n",
       "  0.055943943560123444,\n",
       "  0.05553847551345825,\n",
       "  0.05561830848455429,\n",
       "  0.055769097059965134,\n",
       "  0.055576764047145844,\n",
       "  ...]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3_df = pd.DataFrame(history3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.434180</td>\n",
       "      <td>1.778844</td>\n",
       "      <td>0.766154</td>\n",
       "      <td>0.708265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756223</td>\n",
       "      <td>0.840709</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>0.853611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.762638</td>\n",
       "      <td>0.756714</td>\n",
       "      <td>0.785385</td>\n",
       "      <td>0.564102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.806005</td>\n",
       "      <td>0.492117</td>\n",
       "      <td>0.831538</td>\n",
       "      <td>0.449095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.840390</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.845385</td>\n",
       "      <td>0.367764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.986143</td>\n",
       "      <td>0.053996</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.056187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.983064</td>\n",
       "      <td>0.058399</td>\n",
       "      <td>0.982308</td>\n",
       "      <td>0.051699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.984604</td>\n",
       "      <td>0.057441</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.054310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.983577</td>\n",
       "      <td>0.060338</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.055343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.984347</td>\n",
       "      <td>0.058606</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.052836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy      loss  val_accuracy  val_loss\n",
       "0    0.434180  1.778844      0.766154  0.708265\n",
       "1    0.756223  0.840709      0.763846  0.853611\n",
       "2    0.762638  0.756714      0.785385  0.564102\n",
       "3    0.806005  0.492117      0.831538  0.449095\n",
       "4    0.840390  0.418300      0.845385  0.367764\n",
       "..        ...       ...           ...       ...\n",
       "428  0.986143  0.053996      0.980769  0.056187\n",
       "429  0.983064  0.058399      0.982308  0.051699\n",
       "430  0.984604  0.057441      0.981538  0.054310\n",
       "431  0.983577  0.060338      0.980000  0.055343\n",
       "432  0.984347  0.058606      0.981538  0.052836\n",
       "\n",
       "[433 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = history3_df['loss']\n",
    "y_vloss = history3_df['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss,  \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9840 - loss: 0.0418 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05562335625290871, 0.9815384745597839]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldltest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
